\documentclass[acmsmall,nonacm,screen,review]{acmart}
\newif\ifEnableExtend
%\EnableExtendtrue
\EnableExtendfalse

\usepackage{numprint}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\newcommand{\csch}[1]{{\color{red} Christian says: #1}}
\newcommand{\Is}       {:=}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{relsize}

\newtheorem{openproblem}{Open Problem}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\cov}{\term{cov}\xspace}
\newcommand{\term}[1]{\textsl{#1}}
\newcommand{\Comment}[1]{\textsl{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcopyright{none}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\title{Algorithm Engineering Advanced Practical: Analysing the Impact of Graph Partitioning on Graph Neural Network Training}
\author{Eric Waldherr}
\email{ft278@stud.uni-heidelberg.de, Computer Science,
4231309}
\affiliation{%
  \institution{Heidelberg University}
  \streetaddress{Im Neuenheimer Feld 205}
  \city{Heidelberg}
  \state{Baden-WÃ¼rttemberg}
  \country{Germany}
  \postcode{69120}
}


\date{10th of July 2024}

\begin{document}

\begin{abstract}
Abstract here.
\end{abstract}
\maketitle

\section{Introduction}
This report is structured as follows. First, the fundamental definitions regarding this topic are given in Section 2. Section 3 gives an insight of the used GNN framework and explains the most important training types of it. Furthermore, Section 4 explains in detail how KaHIP was implemented into DGL and other necessary changes made to the framework. The conducted experiment to compare the results between Metis and KaHIP are shown and analyzed in Section 5. Finally, the report closes with an overall picture of the conducted work in Section 6. 
\section{Preliminaries}
This chapter focuses on giving the most important definitions needed to understand the following chapters. Let $G=(V,E)$ be a graph, with $V$ as a set of vertices and $E \subseteq V \times V$ as a set of edges.
\subsection{Graph Partitioning}
The graph partitioning problem is a very fundamental problem in modern computer science as graphs. By partitioning a huge graph first before running a algorithm on it one can run algorithms on the resulting partitions in parallel. This and the quality of the partition itself can have a significant impact of the runtime of an algorithm. \\ 
The input of the problem is a graph $G = (V,E)$ and $k\in \mathbb{N}$. To solve the problem, $G$ needs to be partitioned into $k$ partitions \hbox{$V_{1},...,V_{k}$ such that $V_{1}\cup...\cup V_{k} = V$} and $V_{i}\cap V_{j} = \emptyset\ \forall i,j\in \{1,...,k\}, i \neq j$. The main goal of the partitioning algorithm is to minimize the edge cut, which can be defined as $\vert \{(u,v)\in E : u\in V_{i},\ v\in V_{j} \text{ and } i\neq j \}\vert$. This reduces the amount of communication workload needed between the different workers and can significantly reduce the execution time of operations or algorithms run on the partitioned graph.  \\
To achieve the highest possible parallelization effectiveness, these partitions need to be balanced to distribute the resulting computation workload equally among the computation units. For that reason, the problem includes a vertex balancing restriction: $\forall i\in \{1,...,k\} : \vert V_{i}\vert \ \leq (1 + \delta) \lceil \frac{\vert V \vert }{k} \rceil $, with $\delta \in \mathbb{R}$ called vertex imbalance. Figure \ref{partition} shows a balanced partition. \\
Often, it can be beneficial to balance the edges of the graph as well. This is done by assigning the nodes their outdegree + 1 as weight. The outdegree of a node $v_{i}$ is defined as $deg(v_{i}) = \sum{(u,v)\in E : u = v_{i} \vee v = v_{i}}$.
\begin{figure}[bt!]
\centering
\caption{Example of a balanced partition}
\label{partition}
\includegraphics[width=4.75cm,height=2.5cm]{partition_example.png}
\end{figure}
\section{Related Work}
This chapter focuses on the Deep Graph Learn (DGL) framework \cite{DGL} that was chosen as the GNN framework. In the following, the most important features of DGL are explained that were used to achieve the experimental results of Section 5. In general, DGL provides the possibility to the user to define their own GNN modules. Additionally, one can use the built-in GNN model frameworks. In the course of the experiments GraphSAGE \cite{GraphSAGE} and GraphConv(?) \cite{NC} were used in particular. \\
To process and train on huge graph data sets, DGL utilizes the popular graph partitioner Metis \cite{Metis} per default. With this partitioner the graphs can be partitioned first and then the training can be run on every partition in parallel by the use of multiple CPUs. This can achieve a significant speed up of the whole process. Now the main focus of this report is to investigate how the type and quality of the used graph partitioner can impact the runtime of the training process. For that, the graph partitioner KaHIP \cite{KaHIP} is used. Comparing the results of typical DGL training routines with using Metis and KaHIP can illustrate if the common graph partitioning quality measures also apply for GNNs. \\
In the course of this report two types of DGL training are run for this comparison: node classification and link prediction. These are the most common tasks of DGL and can be explained as follows. Semi-supervised node classification, as defined in detail by Kipf et al. \cite{NC}, is the task of prediction the category of each node. For that, a small amount of labeled nodes of the graph is given to the model determining categories for these sample nodes. The GNN then utilizes these categories together with the connectivity information of the graph to predict the categories for the rest of the nodes. \\%maybe cora as example
Link prediction is the task of predicting whether there is an edge between two nodes in a graph or not. This tasks does not depend on additional sample data as the node classification task. Instead, the input graph itself provides the sample data. A small part of the existing edges of the graph are taken as positive examples. The same number of non-edges from negative examples. Then, the GNN evaluates if there is an edge or an non-edge between two nodes for all the nodes in the graph based on the sampled example data.
\section{Implementation}
This section illustrates how KaHIP was integrated into the DGL framework and what changes needed to be made to DGL to achieve that.
First of all, KaHIP was added to the third party frameworks of DGL. After that, it was included in the building process ensuring it gets compiled correctly. In DGL, most more complex functionalities are realized as follows. There exists a file for each functionality where a function is defined that can be called later outside of that individual file. In that way, a new function was created that calls the main partitioner of KaHIP, named kaffpa. The kaffpa function needs all the conncetivity information of the graph in order to work properly. In detail, it needs the amount of nodes in the graph as well as the source and destiny node for every edge in the graph. This information was extracted by the respective DGL graph class using the already defined methods of that class. Additionally, kaffpa needs to be configured, mainly one of the six modes of that function needs to be chosen. For the start, the FAST mode was chosen by default. At that point of time this was sufficient as the focus lied on getting KaHIP running then, the detailed configurations followed at a later stage.\\
After this first crucial step was done, a crucial problem occurred. All the main information of the graph is given to kaffpa in four arrays of integers. The conflict here was that DGL saves this information in 64-bit arrays while the kaffpa function expects 32-bit arrays. To resolve this conflict, either DGL or KaHIP needed to be changed to accept the integer size of the other component respectively. As changing DGL would result in changing significantly more code, KaHIP was changed instead. This change consisted of changing the function signatures of 21 functions inside of KaHIP distributed over 10 files as well as changing type definitions in its definition-header to accept 64 bit integers for the ids of nodes and edges as well as their weights. These changes achieved their goal and KaHIP could now be called successfully inside of DGL using 64-bit edges and nodes.\\
\section{Experiment}

\section{Conclusion}

\bibliographystyle{plainnat}
\bibliography{references.bib}

\end{document}
