\documentclass[acmsmall,nonacm,screen,review]{acmart}
\newif\ifEnableExtend
%\EnableExtendtrue
\EnableExtendfalse

\usepackage{numprint}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\newcommand{\csch}[1]{{\color{red} Christian says: #1}}
\newcommand{\Is}       {:=}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{relsize}

\newtheorem{openproblem}{Open Problem}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\cov}{\term{cov}\xspace}
\newcommand{\term}[1]{\textsl{#1}}
\newcommand{\Comment}[1]{\textsl{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcopyright{none}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\title{Algorithm Engineering: Analysing the Impact of Graph Partitioning on Graph Neural Network Training}
\author{Eric Waldherr}
\email{ft278@stud.uni-heidelberg.de, Computer Science,
4231309}
\affiliation{%
  \institution{Heidelberg University}
  \streetaddress{Im Neuenheimer Feld 205}
  \city{Heidelberg}
  \state{Baden-Württemberg}
  \country{Germany}
  \postcode{69120}
}


\date{5th of September}

\begin{document}

\begin{abstract}
Over time, deep learning has become an essential field in computer science. Graph Neural Networks (GNNs) are capable of conducting deep learning on graph data. This is crucial, given graphs are a fundamental data structure nowadays and have a variety of applications. Similarly to other algorithms that run on graphs, the training of GNNs can be executed in a distributed and parallel manner. In particular, node classification and link prediction are two essential training types of such structures.\\
This raises the question of how different graph partitioners impact the runtime of GNN training. To address this question, an experiment was conducted. For this purpose, the high-quality partitioner KaHIP was introduced to DGL, a GNN framework. The experiment demonstrates that using node weights to balance partitions can accelerate the training process by a factor of up to 1.61x. Furthermore, the edge cut quality of a partitioner can reduce the time required for a node classification task significantly.
\end{abstract}
\maketitle

\section{Introduction}
Graphs have become a fundamental data structure in modern computer science over the last decades ~\cite{Junghanns2017}. There are many well-researched algorithms that run on \hbox{graphs \cite{Color,SP,Flow}}. Furthermore, machine learning has emerged as a significant field of study in numerous domains of computer science. Graph Neural Networks combine these two fields, representing a form of machine learning that is capable of working with graph data. \\
The training methods of these structures are often memory-intensive. For this reason, many GNN frameworks support distributed training by utilizing graph partitioning algorithms in order to use multiple processing units for the training steps. Moreover, it is well researched that the quality of a graph partitioning algorithm can have a considerable impact on the performance of the subsequent algorithms running on the graph. A well-know example of this is Dijkstra’s Algorithm \cite{DBLP:journals/jea/MohringSSWW06}.\\
This leads to the question of how the quality of a partition can affect the runtime of the subsequent training of a GNN. To address this question, the Deep Graph Learn (DGL) framework \cite{DGL} was used as a basis. DGL employs the widely used graph partitioner Metis \cite{Metis} for the distributed training of GNNs. In this report, the partitioner KaHIP \cite{KaHIP} is added to the DGL library in order to provide a comparison between two partitioners and to investigate their impact on GNN training performance. \\  
This report is structured as follows: first, the fundamental definitions regarding this topic are given in Section 2. Section 3 provides an insight into DGL, the GNN framework used, and explains its most important training types. Furthermore, Section 4 explains in detail how KaHIP was introduced to DGL and how other necessary changes were made to the framework. The conducted experiment to compare the results between Metis and KaHIP is presented and analyzed in Section 5. Finally, the report concludes with an overall picture of the work done in Section 6. 
\section{Preliminaries}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=3.5cm,height = 2 cm]{partition_example.png}
         \caption{Example balanced partition}
         \label{partition}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=2.8cm,height = 2cm]{node_weights.png}
         \caption{Example of node weights}
         \label{weights}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=3.2cm,height = 2cm]{halo_nodes.png}
        \caption{Illustration of halo nodes}
        \label{halo}
     \end{subfigure}
     \caption{Example of basic definitions}
     \label{definitions}
\end{figure}
This section focuses on giving the most important definitions needed to understand the following sections. Let $G=(V,E)$ be a graph, with $V$ as the set of nodes and $E \subseteq V \times V$ as the set of edges. An edge $(u,v)$ with $u,v \in V$ is called a non-edge if $(u,v) \notin E$.
\subsection{Graph Partitioning}
Graph partitioning is a fundamental problem in modern computer science. By initially partitioning a large graph, algorithms can be run on the resulting partitions in parallel. This and the quality of the partitioning itself can have a significant impact on the runtime of an algorithm. \\ 
The input to the problem is a graph $G$ and an integer $k\in \mathbb{N}$. $G$ must be partitioned into $k$ partitions \hbox{$V_{1},...,V_{k}$ such that $V_{1}\cup...\cup V_{k} = V$} and $V_{i}\cap V_{j} = \emptyset\ \forall i,j\in \{1,...,k\}, i \neq j$. The primary objective of the partitioning algorithm is to minimize the edge cut, which can be defined as \hbox{$\vert \{(u,v)\in E : u\in V_{i},\ v\in V_{j} \text{ and } i\neq j \}\vert$.} This has the effect of reducing the amount of communication required between the various workers, which can significantly reduce the execution time of operations or algorithms running on the partitioned graph.  \\
In order to achieve the highest possible effectiveness of parallelization, these partitions need to be balanced to distribute the resulting computation workload equally among the computation units. For this reason, the graph partitioning problem includes a node balancing constraint: $\forall i\in \{1,...,k\} : \vert V_{i}\vert \ \leq (1 + \delta) \lceil \frac{\vert V \vert }{k} \rceil $, with $\delta \in \mathbb{R}$ called node imbalance. An example of a balanced partition is demonstrated in Figure \ref{partition}.\\
A fundamental property of a node in a graph is the amount of edges it is adjacent to, called degree. The degree of a node $v_{i}$ can be defined as $deg(v_{i}) = \vert\sum{(u,v)\in E : u = v_{i} \vee v = v_{i}}\vert$. 
In many cases, it is beneficial to balance the edges of the graph as well. This is done by assigning each node $v \in V$ a weight $w(v) = deg(v) + 1$. Figure \ref{weights} illustrates an example graph where each node is labeled with its respective weight based on its degree plus one. \\
Furthermore, the distributed training of GNNs is not limited to the nodes of a partition itself. Each partition additionally comprises a set of halo nodes. A node $h \in V$ is called a halo node of a partition $V_{i}$ if $\exists (v,h) \in E \vee \exists (h,v) \in E$, for $v \in V_{i}, h \notin V_{i}$. In other words, a halo node of a partition shares an edge with one of the nodes in that partition without being part of the partition itself. These halo nodes are included in the training of each partition to increase the quality of the result. 
\subsection{Graph Neural Network Training}
How GNNs are trained in detail is a highly complex topic. In this instance, only the most important aspects are illustrated. The core of the training of GNNs are features. Each node $v \in V$ gets assigned a node feature $x_{v} \in \mathbb{R}^{d}$, with $d$ called feature size. \\
The training itself consists of multiple steps, where the features of each node are updated based on the features of other nodes to which it is adjacent. Step $t$ of this process can be explained as follows: $x_{v}^{(t)} = \phi(x_{v}^{(t-1)},\rho(\{x_{u}^{(t-1)} : (u,v) \in E \vee (v,u)\in E\}))$. $\rho$ is an aggregation function that takes the features of adjacent nodes as input, while $\phi$ is a function that updates the feature of a node based on its own features and the results of the $\rho$ function.\\ 
During the training, the GNN identifies patterns between the features of the nodes in connection with other relevant node attributes depending on the type of training. After the training is done, a GNN is often used to predict certain attributes of nodes and edges, making it a powerful tool in modern computer science.
\section{Related Work}
This section focuses on the DGL framework that was chosen as the GNN framework as well as another study that conducted a similar research to this report. The following section outlines the key features of DGL that were used to achieve the experimental results presented in Section 5. In general, DGL offers the user the possibility to define their own GNN modules. Additionally, one can use the built-in GNN model frameworks. In the course of the experiments, GraphSAGE \cite{GraphSAGE} and GraphConv \cite{NC} were employed in particular. \\
For processing and training on large graph datasets, DGL uses Metis by default. With this partitioner, the graphs can be partitioned first and then the training can be performed on each partition in parallel by utilizing multiple CPUs. This can achieve a significant speedup of the whole process. The main focus of this report is to investigate how the type and quality of the used graph partitioner can impact the runtime of the training process. For this purpose, the graph partitioner KaHIP is used. Comparing the results of typical DGL training routines with the use of Metis and KaHIP can illustrate if the conventional graph partitioning quality metrics are applicable to GNNs. \\
In the course of this report, two types of DGL training are conducted for this comparison: node classification and link prediction. These are the most common tasks of DGL and can be explained as follows: semi-supervised node classification, as defined in detail by Kipf et al. \cite{NC}, is the task of predicting the category of certain nodes in the input graph. For this purpose, the nodes of the graph are distributed into three subsets: validation, training and testing. \\  
\begin{figure}[bt!]
    \centering
    \includegraphics[width=9.5cm,height = 3.5cm]{gnn.png}
    \caption{Illustration of GNN training process based on node features}
    \label{learning}
\end{figure}
First, the GNN uses the training set to identify patterns between the features and the labels of the nodes in that set. Subsequently, it predicts the labels of the nodes of the validation set based on their features. During this phase, the GNN adjusts its parameters depending on the accuracy of the predictions made. This includes updating the features of each node based on their adjacent nodes, illustrated in Figure \ref{learning}. Finally, the GNN predicts the label of each node of the test set, once more using the features of the nodes. This process is then repeated for a certain amount of epochs. As the number of epochs increases, the accuracy of the predictions of the GNN improves. \\
Link prediction is the process of predicting whether there is an edge between two nodes in a graph or not. This task does not require any additional sample data, such as the labels of the node classification task. In contrast, the input graph itself provides the sample data. A part of the existing edges in the graph are taken as positive examples. Similarly, a portion of the non-edges are taken as negative examples. This data is used to minimize the prediction error of the GNN over a certain amount of epochs. Ultimately, the GNN is capable of predicting the existence of an edge or a non-edge between two nodes for all the potential edges in the graph.\\
Merkel et al. \cite{Other} conducted an analysis to determine the impact of varying GNN parameters and graph types on the efficacy of graph partitioning. The results demonstrated that an increase in the feature size of the nodes enhances the efficacy of partitioning, whereas other parameters have a decreasing effect on it or do not affect it significantly. Furthermore, it was discovered that high-quality graph partitioning can accelerate the training process of GNNs. This study analyzes whether the results on the impact of high-quality graph partitioning can be reproduced using the two GNN training types described above specifically.
\section{Implementation}
This section illustrates how KaHIP was integrated into the DGL framework and what changes needed to be made to DGL to achieve that.
\subsection{Integration of KaHIP to DGL}
First of all, KaHIP was added to the third-party frameworks of DGL. After that, it was included in the building process, ensuring it gets compiled correctly. In DGL, the majority of complex functionalities are realized as follows: a file is created for each functionality, wherein a function is defined that can be called at a later stage outside of that individual file. Therefore, a new function was created that calls the main partitioner of KaHIP, named kaffpa. The kaffpa function requires all the connectivity information of the graph in order to work properly. In particular, the kaffpa function requires the amount of nodes in the graph as well as the source and destination nodes for each edge in the graph. This information was extracted by the respective DGL graph class utilizing the predefined methods of that class. Additionally, kaffpa must be configured. Essentially, one of the six available modes of the function must be selected. The FAST mode was selected by default as a preliminary step. At that point in time, this was sufficient, as the objective was to initiate the operation of KaHIP. The detailed configurations were implemented at a later stage.\\
After this first fundamental step was done, a crucial problem occurred. All the essential information of the graph is given to kaffpa in four arrays of integers. The conflict here was that DGL saves this information in 64-bit arrays, whereas the kaffpa function expects 32-bit arrays. To resolve this conflict, it was necessary to modify either DGL or KaHIP to accept the integer size of the other component, respectively. As modifying DGL would result in changing significantly more code, KaHIP was adjusted instead. This modification involved changing the function signatures of 21 functions inside of KaHIP, distributed across 10 files, as well as modifying the type definitions in its definition header to accept 64-bit integers for the IDs of nodes and edges as well as their weights. These changes were successful in achieving their objective, whereby KaHIP could now be called successfully within DGL using 64-bit edges and nodes. \\
The next step was to include this new functionality in the corresponding partition utility file. In this instance, an additional setup and a post-processing step are necessary before and after the actual partitioning function is called. This partitioning function was implemented as described above. The initial step is to transform the graph into a bidirectional graph. This means that for each $(u,v) \in E$ the corresponding backward edge $(v,u)$ is added to $E$. This is a standard initial step that is done before partitioning in DGL and is also done before every partition call of Metis. This process is illustrated in Figure \ref{bidirecting}. Then, after the partitioning process concludes, the halo nodes of each partition are identified. Subsequently, these nodes are added as to the partitions' halo nodes, respectively. This step in the implementation opened the possibility to call a KaHIP partition in DGL and then store the result and continue working on the different partitions at a later stage. \\
Even though a KaHIP partition could be conducted at this stage of development, the feature to actually utilize the partition in a distributed and parallel context remained to be implemented. A function for this purpose had already been provided by DGL, but thus far only for a Metis partition. The default behaviour of the function was to call a Metis partition. Each partition of the result was then stored in a destination directory in its own file. These individual partitions can then be loaded independently and then used by different computation units in parallel. The option of selecting either Metis or KaHIP when calling this function was enabled by adding an additional parameter that determines which partitioner to call. This concluded the integration of KaHIP into the DGL framework and each of the two partitioners could be called by the user.
\subsection{Additional Changes for Experiment}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=3.45cm,height = 2 cm]{directed_graph.png}
         \caption{Original graph}
         \label{uni}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=3.45cm,height = 2cm]{bidirectional_graph.png}
         \caption{Graph after adding backward edges}
         \label{bi}
     \end{subfigure}
     \caption{Process of bidirecting a graph}
     \label{bidirecting}
\end{figure}
Further features were added to the partitioning process of DGL to provide a more thorough analysis of the conducted experiments. Edge balancing is a crucial quality metric for the graph partitioning process. Accordingly, the subsequent objective was to add $deg(v) + 1$ as a weight to each node $v \in V$. This was accomplished by including an additional setup step before calling a partition, where the information of the degree of each node was extracted from the graph. This was achieved through a function, provided by DGL, that returns a tensor. Each entry of that tensor represents the degree of a node in the graph. Then the value 1 was required to be added to each of those entries. This tensor could then be utilized by handing it to the partitioners by an additional parameter. As this method of edge balancing is a common one, the partitioners are able to accept these arrays and are able to return a partition that is balanced based on that array. This feature was introduced to further investigate the influence of conventional partition quality metrics on the performance of GNNs. Moreover, a parameter was added to enable or disable this feature for different partitioning ~executions.  \\
KaHIP provides six distinct execution modes in total. The available modes are FAST, ECO, STRONG, FASTSOCIAL, ECOSOCIAL and STRONGSOCIAL. At the current implementation stage, only the FAST mode was available, as previously stated in Section 4.1. To be able to choose from these modes, an additional parameter was introduced to all of the functions that include the KaHIP partitioning call. This allows for an insight into the impact of the different modes in the experiment.

\section{Experiment}
\begin{table}[bt!]
\centering
\begin{tabular}{ ccc }
 \centering
  & Node Classification & Link Prediction \\ 
 \hline
 Distribution & single & fat \\
 CPUs & \multicolumn{2}{c}{Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz} \\ 
 \# of CPUs & 32 & 32 \\ 
 Memory & 16GB & 356GB \\
\end{tabular}
\caption{Specifications of the used cluster}
\label{hw}
\end{table}
\begin{table}[bt!]
\centering
\begin{tabular}{ cccccc }
 \centering
 Graph & $|V|$ & $|E|$ & Node Feature Size & \# of Node Categories \\ 
 \hline
 Questions \cite{TQ} & 49k & 307k & 301 & 2 \\ 
 Flickr \cite{Flickr} & 89k & 900k & 500 & 7\\ 
 Tolokers \cite{TQ,Tolokers} & 12k & 1M & 10 & 2 \\

\end{tabular}
\caption{Graphs used for the experiment}
\label{graphs}
\end{table}
This section provides a comprehensive description of the experimental environment. Afterwards, the results of the experiments are presented and evaluated. This includes a comparison of Metis and KaHIP performing a graph partition followed by the same subsequent GNN training. The execution times of the training are compared for different partitioning sizes and node weights between Metis and all the available KaHIP execution modes. 
\subsection{Experimental Environment}
The experiment was conducted on the BwUniCluster2.0 computing cluster. In the case of node classification, the single distribution was employed, whereas for link prediction, the fat distribution of the cluster was utilized. The configuration of the cluster employed in the experiment is presented in detail in Table \ref{hw}.\\
Three graphs were used for the experiment in total. The sizes, feature specifications, and number of distinct node categories for each graph are detailed in Table \ref{graphs}. It is notable, that the original Tolokers and the Questions graph only have half of the edges shown in Table \ref{graphs}. DGL already included these two graphs into their example graph data sets and added all the backward edges for each edge in the original graph.\\
The two training types were executed as follows: The initial step is to partition the graph. Subsequently, the actual training for each partition is conducted in parallel using PyTorch \cite{PyTorch} and 100 epochs. The PyTorch library provides functions to execute GNN training in parallel through the creation of a new process for each partition, which was utilized for the experiment. The training was repeated three times for each instance. Subsequently, the mean of the three runs was determined and used for the diagrams of the following subchapters. Each graph was partitioned twice using the same configurations (for node classification and for link prediction). The average edge cut produced out of these two partition processes was taken for the edge cut results, shown in Figure \ref{cut}.
\subsection{Evaluation of Node Classification Results}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{edge-cut_4.png}
         \caption{Edge cut for 4 partitions}
         \label{cut_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{edge-cut_8.png}
         \caption{Edge cut for 8 partitions}
         \label{cut_8}
     \end{subfigure}
     \caption{Produced edge cut of the algorithms}
     \label{cut}
\end{figure}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{nc_in_4.png}
         \caption{Partitioning time included}
         \label{nc_in_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{nc_ex_4.png}
         \caption{Partitioning time excluded}
         \label{nc_ex_4}
     \end{subfigure}
     \caption{Execution time of node classification using 4 partitions}
     \label{nc_results}
\end{figure}
\begin{table}[bt!]
\centering
\begin{tabular}{ cccc }
 \centering
  Mode & Flickr  & Tolokers & Question \\ 
 \hline
    FAST &	318	& ---	&--- \\
    ECO	&1686	&---&--- \\
    STRONG	&11831	&---	&44760\\
    FASTSOCIAL	&1	&---	&1 \\
    ECOSOCIAL	&6	&---	&4734 \\
    STRONGSOCIAL	&2100	&550200	&111967\\
\end{tabular}
\caption{Required number of epochs of node classification until Metis is outperformed using node weights}
\label{nc_epochs}
\end{table}
In this phase of the experiment, the graphs were primarily partitioned into four parts. The produced edge cut is listed in Figure \ref{cut_4}. In the majority of cases, the KaHIP modes FAST and ECO resulted in the highest edge cut. Then, Metis and the modes FASTSOCIAL and ECOSOCIAL follow. The best edge cut was achieved by the STRONG and STRONGSOCIAL modes of KaHIP. \\ 
However, these low edge cuts result in very high partitioning times, leading to poor execution times, as illustrated in Figure \ref{nc_in_4}. This figure shows the execution times for each algorithm and for each graph with and without node weights. Additionally, this illustration demonstrates that overall the execution time of Metis and the FASTSOCIAL mode of KaHIP performed the best.  \\
Figure \ref{nc_ex_4} presents the execution times as Figure \ref{nc_in_4}, but without including the time required by the partitioning algorithms to complete. This diagram illustrates the actual impact of the graph partitioning on the GNN training time. The results demonstrate that the training process following a Metis partition is outperformed by the STRONG, FASTSOCIAL, ECOSOCIAL and STRONGSOCIAL modes of KaHIP in the majority of cases. These four modes achieved superior edge cut quality in comparison to Metis. This implies that the edge cut metric has a significant impact on GNN node classification training. In the experiment, 100 epochs were conducted for the training. Generally, the accuracy of GNNs is increasing with an increasing amount of epochs. Considering this fact makes it clear that the impact of graph partitioning on GNN training time increases with more epochs of training. Table \ref{nc_epochs} lists how many epochs of training are required such that the higher partitioning time of the KaHIP modes is amortized by the training acceleration. In other words, this table shows how many epochs of training are needed for the different modes to outperform Metis. \\
A closer examination of the training times and the resulting edge cut reveals that the FASTSOCIAL and ECOSOCIAL modes perform best in most instances. However, the edge cut of the STRONGSOCIAL and STRONG modes are better. This suggests that there may be additional partition quality metrics that are specific to GNNs.\\
Another aspect Figure \ref{nc_ex_4} illustrates is that the usage of node weights to achieve balanced partitioning has a considerable positive impact on the training of GNNs. In the majority of instances, including node weights decreased the training time of the GNN. On average using node weights led to a speedup of the training of 1.20x for the Questions graph and 1.12x for the Tolokers graph. The training times of the Flickr graph were not impacted noticeably by node weights. This marks another quality metric of graph partitioning that has a positive effect on GNN training.\\
Figure \ref{nc_comp} reports the execution time for node classification on the Flickr graph for varying numbers of partitions, excluding the time required for graph partitioning. As the number of partitions increases, the execution time is reduced. This illustrates that an increase in the number of partitions leads to a greater degree of parallelization and highlights the importance of graph partitioning in general. This speedup is expected to be even higher when executing node classification tasks on larger graph datasets. Additionally, it is notable that for four partitions only the STRONGSOCIAL mode of KaHIP produces superior training times in comparison to Metis. Increasing the amount of partitions to 16 leads to three additional KaHIP modes outperforming Metis.\\
Overall, the results of the node classification training demonstrate that this task is affected positively by common partition quality metrics. However, the optimal choice for a partitioning algorithm seems to be dependent on a number of factors, including the number of partitions and the size of ~the graph.
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=4.1cm,height = 3 cm]{nc_comp.png}
         \caption{Node classification runtime on the Flickr graph}
         \label{nc_comp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=4.1cm,height = 3cm]{lp_comp.png}
         \caption{Link prediction runtime on the Tolokers graph}
         \label{lp_comp}
     \end{subfigure}
     \caption{GNN training time using different numbers of partitions}
     \label{parts}
\end{figure}
\subsection{Evaluation of Link Prediction Results}
For this task, the main experiments were conducted using eight partitions. The resulting edge cuts are presented in Figure \ref{cut_8}. The algorithms performed similar to the edge cut results observed in Figure \ref{cut_4}. The STRONG and STRONGSOCIAL modes of KaHIP produce the best edge cut, while the FAST and ECO modes exhibited the least favourable performance in this regard. \\
Looking at the execution time of the whole procedure, illustrated in Figure \ref{lp_in_4}, the relatively high partitioning time of the STRONG and the STRONGSOCIAL modes resulted in suboptimal execution times. The most effective algorithms were Metis and the FASTSOCIAL mode. \\
These two algorithms also have the greatest impact on training time. This can be observed in Figure ~\ref{lp_ex_4}. Furthermore, the impact of the edge cut on the training times is smaller for the link prediction task than observed for the node classification. In some instances, the STRONG and STRONGSOCIAL modes perform poorly despite their excellent edge cut quality. Moreover, the performance of each partitioning algorithm differs significantly for different instances. Therefore, it is essential to select a graph partitioner carefully, depending on the exact use case. \\
Comparing the entries of Table \ref{nc_epochs} and Table \ref{lp_epochs} demonstrates that high-quality partitioning affects link prediction less than node classification, given a majority of entries in Table \ref{lp_epochs} are empty. For these instances, the higher partition quality of the KaHIP modes did not result in an acceleration in training compared to Metis. Considering that link prediction is a edge-based task leads to the speculation that using a edge partitioner instead, could accelerate this training type more than Metis and KaHIP.\\
The addition of node weights has a beneficial effect on training outcomes, with the greatest impact observed in the case of the Questions graph in Figure \ref{lp_ex_4} with a speedup of 1.61x on average across all algorithms. Node weights accelerated the Tolokers graph training by 1.10x on average, while no significant difference was found for the Flickr graph. As this acceleration can be observed for both training types, the edge balance quality metric appears to be a fundamental aspect of GNN training. \\
The link prediction was conducted for varying numbers of partitions on the Tolokers graph. The resulting GNN training times are illustrated in Figure \ref{lp_comp}. In the majority of instances, increasing the amount of partitions led to a reduction in the time taken to complete the training process. The number of partitions influences the performance of the training process significantly. This can be observed in the unweighted node case. In that case, using four partitions results in the FAST and ECO modes of KaHIP performing poorly, but for 16 partitions they perform the best. This aspect reinforces the assumption that the partitioner should be selected based on the specific use case.\\
Overall, this experiment demonstrates that the choice of graph partitioning algorithm has a significant impact on the performance of GNN training. Furthermore, for the link prediction tasks, the FASTSOCIAL mode of KaHIP is the optimal choice in the majority of instances.
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{lp_in_8.png}
         \caption{Partitioning time included}
         \label{lp_in_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{lp_ex_8.png}
         \caption{Partitioning time excluded}
         \label{lp_ex_4}
     \end{subfigure}
     \caption{Execution time of link prediction using 8 partitions}
     \label{lp_results}
\end{figure}
\begin{table}[bt!]
\centering
\begin{tabular}{ cccc }
 \centering
  Mode & Flickr  & Tolokers & Question \\ 
 \hline
    FAST	&---	&---&--- \\
    ECO	&---	&---	&64 \\
    STRONG	&---	&---	&5900 \\
    FS	&69	&---&1 \\
    ES	&---	&---	&44 \\
    SS	&---&---&---\\

\end{tabular}
\caption{Required number of epochs of link prediction until Metis is outperformed using node weights}
\label{lp_epochs}
\end{table}
\section{Conclusion}
In this study, the high-quality partitioner KaHIP was introduced to the DGL framework. The experiment conducted demonstrates that distributed GNN training is impacted significantly by the enforced graph partitioner. The use of the degree of a node as a weighting factor, thus balancing the edges for the partitions, has been found to improve subsequent training time. Moreover, it was found that fundamental partition quality metrics influence GNNs in a positive manner.\\
However, in some instances, a high-quality partition did not result in a superior execution time for the training. This leads to the assumption that there are more metrics that affect the training of GNNs, which differ from common partitioning quality metrics. Further investigation into this possibility and the identification of more accurate metrics for GNNs could lead to the development of graph partitioners based on these metrics. This could potentially lead to a significant acceleration of GNN training in the future.\\
Furthermore, the experiment made clear that different types of partitioners can be advantageous, depending on the specific use case of GNN training. Consequently, future developments in GNN frameworks like DGL should include the introduction of additional graph partitioners. Consequently, this would allow the users to select the partitioner based on their GNN training, leading to the optimal execution times.
\section{Acknowledgments}
The author would like to express gratitude to Adil Chhabra, Ernestine Grossmann, Kenneth Langedal and Christian Schulz for their supervision and support of the project. Furthermore, the author acknowledges the support provided by the state of Baden-Württemberg through BwHPC.
\bibliographystyle{plainnat}
\bibliography{references.bib}

\end{document}
