\documentclass[acmsmall,nonacm,screen,review]{acmart}
\newif\ifEnableExtend
%\EnableExtendtrue
\EnableExtendfalse

\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\newcommand{\csch}[1]{{\color{red} Christian says: #1}}
\newcommand{\Is}       {:=}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{relsize}

\newtheorem{openproblem}{Open Problem}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\cov}{\term{cov}\xspace}
\newcommand{\term}[1]{\textsl{#1}}
\newcommand{\Comment}[1]{\textsl{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcopyright{none}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\title{Algorithm Engineering Seminar: }
\author{Eric Waldherr}
\email{ft278@stud.uni-heidelberg.de, Computer Science,
4231309}
\affiliation{%
  \institution{Heidelberg University}
  \streetaddress{Im Neuenheimer Feld 205}
  \city{Heidelberg}
  \state{Baden-WÃ¼rttemberg}
  \country{Germany}
  \postcode{69120}
}


\date{10th of July 2024}

\begin{document}

\begin{abstract}
Graph data and parallelization of algorithms have become more and more important over time. Nowadays, graphs are used to store data everywhere, and the graphs themselves get larger as well. This leads to the rising importance of graph partitioning. In this report, a label propagation based algorithm is examined that differs from the original label propagation approach. This algorithm achieves very high performance, especially on limited memory resources, where it outperforms popular partitioning frameworks in terms of speed and balance.
\end{abstract}
\maketitle

\section{Introduction}
Over the last decades, graphs have become a crucial data structure to save data in the modern era of computer science. Companies like Google, Facebook, or Amazon use graphs with millions to billions of vertices and edges to save and compute data. \\ 
This trend of graphs getting larger and larger leads to the problem that a singular machine cannot run computations on these huge data sets due to a lack of memory. This is why the graph partitioning problem became more and more present over time, as these huge graphs need to be partitioned in order to run algorithms on multiple machines in parallel. For that reason, every algorithm that is computed on huge data sets and every parallel algorithm on graphs faces the graph partitioning problem at some point. \\
This report summarizes the contents of the paper "Label Propagation-Based Parallel Graph-Partitioning for Large-Scale Graph Data" \cite{Main} and evaluates the proposed graph partitioning algorithm in that paper. First, the structure of the graph partitioning problem is explained in detail in Section 2. Then an overview of different approaches to solving the problem is presented in a high-level view in Section 3. Section 4 provides a detailed explanation of the algorithm proposed in the paper. Then the experimental environment is presented in which the algorithm was compared to other partitioners, followed by an evaluation of the algorithm in Section 5. Finally, this report concludes with an overall picture of the algorithm. in Section 6.
\section{The Graph Partitioning Problem}
\begin{figure}[t]
\centering
\caption{Example of a balanced partition}
\label{partition}
\includegraphics[width=0.5\textwidth,height=3cm]{partition_example.png}
\end{figure}
This chapter focuses on explaining the graph partitioning problem and giving the important definitions needed to understand the following chapters. \\
The input of the problem is a graph $G = (V,E)$ and $k\in \mathbb{N}$. To solve the problem, $G$ needs to be partitioned into $k$ partitions $V_{1},...,V_{k}$ such that $V_{1}\cup...\cup V_{k} = V\ and\ V_{i}\cap V_{j} = \emptyset\ \forall i,j\in \{1,...,k\}$.\\
An important aspect is the edge cut quality, both with unweighted and weighted graphs. That means the partitioning algorithm needs to minimize the edge cut, which can be defined as $\vert \{(u,v)\in E : u\in V_{i},\ v\in V_{j}\ and\ i\neq j \}\vert$. This reduces the amount of communication workload needed between the different workers and can reduce the execution time of operations or algorithms run on the partitioned graph significantly. \\
As partitioning is often done to prepare a graph for a parallel algorithm, these partitions need to be balanced to distribute the resulting computation workload equally among the computation units. For that reason, the problem includes a vertex balancing restriction: $\forall i\in \{1,...,k\} : \vert V_{i}\vert \ \leq \alpha \lceil \frac{\vert V \vert }{k} \rceil $, with $\alpha \in \mathbb{R}$ called vertex imbalance. \\
Many algorithms include computations that focus on the edges of a graph. Therefore, a edge balancing restriction is common to further balance the computation workload: $\forall i\in \{1,...,k\} : \vert (u,v)\in E : u or v \in V_{i} \vert \ \leq \beta \lceil \frac{\vert E \vert }{k} \rceil $, with $\beta \in \mathbb{R}$ called edge imbalance.
\section{Different Approaches to solve the graph partitioning problem}
The following subchapters give an overview of common approaches to solving the graph partitioning problem from a high-level perspective.
\subsection{Local search score based approach}
This approach often divides the graph into k partitions at random initially.
Then, for every vertex, the number of edges inside and outside the same partition is computed. With that information, it is clear whether a node should stay in its current partition or if it should be relocated to another partition. This process is repeated until no more relocations of vertices that would improve the quality of the partition can be found. This general idea is used in the FM \cite{FM} and the KL \cite{KL} algorithms, which are popular score based approaches. \\
This approach has the problem that it gets stuck in a local optimum easily. In addition to that, there could be a beneficial swap of vertices for the edge cut quality, even if the computation says otherwise.
\subsection{Multilevel approach}
This approach consists of three phases. \\
First there is the coarsening phase. This reduces the size of the graph by contracting connectivity information. All of this information needs to be saved in order to get the whole graph together again in the later stages of the algorithm. For that reason, one disadvantage of the multilevel approach is the high memory usage, which is important in the later chapters of this report. \\
Then the next phase begins, which is the initial partition step. Now the reduced graph is partitioned into k parts. After this step is done, what is left to do is take the other vertices that have been ignored so far into consideration. \\
This is done in the final step, the uncoarsening phase. This step often includes heuristics and can include a swapping process that is similar to the swapping process of the score based approach. Now the original graph is reconstructed with the saved connectivity information. This phase ends with the whole graph partitioned into k partitions.
\subsection{Label Propagation approach}
A very detailed view of a Label Propagation approach will be given with the algorithm proposed in the paper. For that reason, this chapter only describes the algorithm from a high-level perspective. \\
Initially, the graph is partitioned over the given machines. The LP approach consists of many iterations. In each iteration, all vertices are assigned a score that describes whether the node should be swapped into another partition or stay in the current partition. Then the vertices with an appropriate score are relocated to another partition. This is often done until there is no more improvement to be found by using the scores. \\
The original LP approach does not guarantee a balanced partition. Balancing restrictions are often needed in modern applications, making this fact a disadvantage of the original LP approach.
\section{Detailed description of a Label Propagation based algorithm}
This section explains the technical details of the proposed algorithm in the paper \cite{Main}. The algorithm is a Label Propagation based approach to solving the graph partitioning problem. The algorithm consists of three phases. In the following subchapters, the initial computations of the algorithm as well as the three phases are explained in detail.
\subsection{Definitions}
First of all, there are a few definitions that are essential for understanding the algorithm that are presented in this chapter. To every vertex, a Vertex Partition Score (VP Score) can be assigned. The VP Score of vertex $v_{i}$ \ is defined as $VP\ Score(v_{i})\ =\ v_{il}\ -\ v_{ir}$ , where $v_{il}$ is the number of edges of $v_{i}$ to vertices inside of the same partition (local edges) and $v{ir}$ is the number of remote edges, which are edges to vertices of other partitions (remote edges). Generally, the higher the VP Score of a vertex, the more appropriate it is in the current partition it is in. This is the case because a high VP Score means a high amount of local vertices, per definition. Relocating such a vertex would result in a high increase in edge cut, as all the edges to local vertices would then be included in the edge cut. At the same time, this means that relocating a vertex with a low VP Score likely decreases the edge cut. \\
To measure the edge cut quality of the graph, the Total Score (T Score) is calculated. This represents the quality of the current partition and is defined as $T\ Score\ =\ \sum_{v\in V} VP\ Score(v)$. It is notable that when the edge cut of the partition decreases, the T Score increases as it is connected to the amount of remote vertices that are in a graph. That means that a higher T Score indicates a higher edge cut quality. \\
The algorithm only focuses on the most important vertices per step, called candidate vertices. In the first phase, the QCLP process, candidate vertices are the vertices that are inside the set of local lower score vertices (LLSV). The LLSV of a partition $p_{i}$ is defined as $LLSV(p_{i})\ =\ \{ m\in \mathbb{N}\ \text{vertices with the }m\text{-lowest scores in}\ p_{i}\}$, where $m$ is decreasing during the course of the algorithm. \\
During the second phase of the algorithm, candidate vertices are vertices that are inside the set of high connectivity to remote vertices (HCRV). How this set of vertices is determined is complex and is therefore explained in detail in Section 4.4.
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth,height = 4 cm]{score_example.png}
         \caption{Example VP Score}
         \label{vp_score}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=\textwidth,height = 4cm]{datastructure.png}
         \caption{Used data structure}
         \label{data_structure}
     \end{subfigure}
     \caption{Illustration of VP Score and data structure}
     \label{combi}
\end{figure}
\subsection{Data structure}
To understand the algorithm in detail, it is important to have the used data structure in mind. In this section, the used data structure will be explained and can be seen in Figure \ref{datastructure}. \\
In general, the algorithm uses two types of data: shared and local. Initially, the graph is randomly distributed among the machines used for the partition. The resulting information about the vertices and edges of each machine makes up the local data of that individual machine. It is important to state that this initial partition is immutable, meaning that once the vertices are distributed among the machines, they are not relocated to the local data of another machine. Only the information that they swapped partitions is passed. The shared data, meaning information that is shared among all machines through communication, consists of the LLSV or HCRV of each partition and a table that contains the information for each vertex, which partition, and which VP Score it is currently assigned to.
\subsection{Quick Converging Label Propagation}
This process is the main stage of the algorithm and is described in detail in this subchapter. This phase is deducted from every machine used independently. The first detail lies in the calculation of the VP Score. Initially, the VP Scores of all vertices are calculated. In each of the following iterations, only the VP Score of the vertices in LLSV and their neighbors is recalculated. This differs from the original LP approach, where the score of all vertices is recalculated each iteration. Here, this is only done on the stated vertices to save computation time, which is crucial in this step of the algorithm. Also, the calculation of the VP Score ignores vertices that are currently part of the LLSV. This is done as the vertices in the LLSV are likely going to be relocated in the next iteration. Thus, including them in the calculation would lead to decisions made based on data that could be outdated. \\
Another important thing to take a look at is the communication structure used for this algorithm. It is called the bulk synchronous parallel style \cite{BSP} lazy update scheme. This is supposed to solve the problem of the trade-off that is occurring here. More communication increases the communication overhead but at the same time improves the calculation of the VP Score, thus leading to a higher partition quality. This communication paradigm  results in the fact that only crucial information is shared when significant changes are made or at critical steps of the algorithm. \\
This phase is called Quick Converging Label Propagation because the size of LLSV is reduced every iteration, ensuring that it converges quickly. Each iteration of this phase works as follows: First, the size of the LLSV is decreased. Then the vertices for the new LLSV are found, and the LLSV is shared with the other machines. Afterwards, the vertices of the LLSV are relocated. Then, the VP Score is calculated for LLSV and its neighborhood. Then, the updated vertex location is shared with the other workers as well as the VP Scores of the neighbors of the relocated vertex. The VP Scores of vertices in the LLSV are then updated again, as many of them have changed partitions. Finally, the local T Score is calculated and shared, making up the overall T Score with the results of the other workers. This ends the current iteration, and the next begins. Figure \ref{iteration} illustrates the most important steps of each iteration. \\
In general, this phase is done for at least $\alpha$ iterations. After that limit is reached, this phase continues until the T Score has not improved for $\beta$ iterations for $c$ percent. The parameters $\alpha,\beta$ and $c$ are user-specified. \\
In conclusion, this phase does the most work for the partition and also keeps near-perfect vertex balance in the process. However, it is important to note that this phase does not lead to an optimal solution and can end up getting trapped in a local optima. Also, this phase only ensures an edge balance within two times of the edge balance restriction. Both of these issues are addressed by the following phases of the algorithm.
\begin{figure}[t]
\centering
\caption{Illustrate of a QCLP iteration}
\label{iteration}
\includegraphics[width=\textwidth,height=6cm]{qclp_iteration.png}
\end{figure}
\subsection{Stabilization phase}
This second phase of the algorithm is designed to resolve the potential problem that the QCLP phase got trapped in a local optimum. This phase works almost the same as the QCLP phase. The main difference is that now nodes of the HRCV are relocated instead of nodes of the LLSV. The stabilization process also follows the same idea of the VP Score which is calculated as known by the QCLP. The difference is that now only vertices with a VP Score less than zero are considered at all. The HRCV is made up of the vertices with the highest number of remote vertices. But only vertices, where the partition they would be relocated to does not already exceed the edge/vertex imbalance ratio, end up in the final HRCV. Furthermore, vertices that would be relocated to a partition that would then exceed the vertex imbalance are removed from the HRCV. This imbalance ratios are now taken into consideration, as this phase prevents the imbalances from increasing further. \\
Each iteration of this phase works the same as an iteration of the QCLP phase. The only difference is that the candidate nodes are found differently, as described. Therefore, Figure \ref{iteration} also illustrates an iteration of this stage after exchanging LLSV for HRCV, respectively. This phase runs through a fixed amount of $\gamma$ iterations, which is user-specified. \\
Overall, this phase maintains the near-perfect vertex balance achieved in the QCLP phase while also preventing a further increase in edge imbalance. Also, the local optima problem is resolved, and the edge cut is improved even further.
\subsection{Edge balancing partitioning process}
This final phase of the algorithm serves the purpose of maintaining the edge/vertex imbalance restriction while also keeping the near-perfect vertex balance achieved in the preceding phases. To achieve the needed edge balance, this phase ignores the edge cut quality completely and focuses on the balance only. Partitions with too many edges relocate vertices randomly to partitions that are below the edge limit in descending order of outdegree. This process is repeated until the edge balance restriction is fulfilled. Therefore, this phase has a negative impact on the edge cut, as this process does not take the edge cut into consideration. However, this ensures that all balancing restrictions are fulfilled. This marks the end of the algorithm. Overall, the algorithm solves the partitioning problem with a high focus on keeping the balancing restrictions as well as trying to achieve a high edge cut quality during the iterations of the first two phases.
\section{Evaluation of the algorithm}
\begin{table}[bt!]
    \centering
    \begin{tabular}{ccc}
    \hline
     & Small Cluster & Large Cluster \\
    \hline
     Instance & c5.2xlarge & r4.8xlarge\\
     CPU & 3.90 GHz & 2.30 GHz\\
     Network Bandwidth & maximum 10Gb/s & 10Gb/s\\
    Memory  & 16GB & 44GB\\
     \# of workers(cores) & 4(4*4= 16) & 16(16*16=256)\\
     \hline
    \end{tabular}
    \caption{Specifications of used clusters}
    \label{clusters}
\end{table}
\begin{table}[bt!]
    \centering
    \begin{tabular}{ccc}
    \hline
    Name & \# of Vertices & \# of Edges \\
    \hline
     Pokec & 1M & 30M\\
     LiveJournal & 4M & 68M\\
     LiveJournal-link & 5M & 49M \\
     Enwiki-2013 & 4M & 101M \\
     Orkut & 3M & 117M\\
     DBpedia-link & 18M & 172M \\
     Wikipedia-link & 12M & 378M\\
     Twitter & 41M & 1.4B\\
     \hline
    \end{tabular}
    \caption{Graphs used for the experiment}
    \label{graphs}
\end{table}
The described algorithm in Section 4 has been benchmarked by experiments and comparisons to other partitioners. In this section, the experimental structure will first be explained. Then, the results of those experiments will be presented. This chapter closes with ideas that could improve the algorithm.
\subsection{Experiment}
First of all, the other partition frameworks used for the experiment will be presented. Metis \cite{Metis} and ParMetis \cite{ParMetis} are known high-quality partitioners. These partitioners are used in a variety of experiments as they are known for their speed and high quality. While Metis only works on single machines, ParMetis is a parallel version of Metis. They are used here as they are popular partitioners that use the multilevel approach, described in Section 3.2. This provides an excellent comparison to the very widely spread multilevel approach. In addition to that, XtraPuLP \cite{XtraPuLP}, which is a parallel version of PuLP \cite{PuLP}, is used to provide a comparison to another LP-based partition approach. All of the partitioners were run with a vertex imbalance of 1.03 and an edge imbalance of 1.30. \\
Furthermore, two different clusters were used for the experiment, a smaller one and a large one. Their properties are listed in Table \ref{clusters}. Also, eight different graphs of different sizes were partitioned in total, and their size is noted in Table \ref{graphs}. The seven smaller graphs were partitioned on the small cluster, while the three largest graphs were run on the large cluster. \\
Looking at the execution times of the algorithms, it is clear that the proposed algorithm runs fastest, followed by XtraPuLP, and then, with a significant distance, Metis and ParMetis finish. Another thing that can be noticed immediately is that Metis and ParMetis did not compute for the two largest graphs running on the small cluster. This and the high difference in runtime can be explained by the fact that the multilevel approach has high memory consumption, as described in Section 3.2. The clusters provide only a very limited amount of memory per vertex, leading to the measured struggle of the two multilevel partitioners. Additionally, the proposed algorithm is the only framework able to complete the partition of the largest graph on the small cluster, which is not shown in the figures. \\
Taking a look at the edge cute quality, it becomes clear that the proposed algorithm is close to the quality of Metis and is outperforming XtraPuLP for the larger graphs. This shows that the algorithm is capable of producing high-quality partitions. \\
Looking at the balancing restrictions, it is clear that ParMetis is the only partitioner that exceeds the limits significantly, while XtraPuLP shows the same behavior for edge balancing. Apart from that, all partitioners were able to fulfill the restrictions. \\
Overall, the experiments on the small cluster show the excellent scalability of the proposed algorithm, as it outperforms the other partitioners on the limited resources of the small cluster. Achieving high scalability is also the main focus of the algorithm, marking this as a great success of development. \\
When looking at the experiments on the larger cluster, you notice that the results are very similar in the fields of execution time and balancing. Taking into consideration that the memory per vertex on the large cluster is still very limited, it is clear why Metis and ParMetis perform very poorly in terms of runtime. While having a similar edge cut as Metis on the large cluster as well, the proposed algorithm shows lower edge quality than XtraPuLP on the largest graph.
\begin{figure}[t]
\centering
\caption{Results of experiment on small cluster}
\label{small_ex}
\includegraphics[width=\textwidth,height=6cm]{small_cluster_experiment.png}
\end{figure}
\begin{figure}[t]
\centering
\caption{Results of experiment on large cluster}
\label{large_ex}
\includegraphics[width=\textwidth,height=6cm]{large_cluster_experiment.png}
\end{figure}
\subsection{Evaluation and possible improvements}
The results of the experiment make clear that the main goal of the proposed algorithm, which was producing a partitioner for large-scale graph data, was successful. Especially on clusters with limited memory resources, the proposed algorithm provides excellent execution time and balancing and outperforms other partitioners while maintaining a high edge cut quality. A comparison of clusters that are more sufficient for the multilevel approach would be very interesting and could show whether the proposed algorithm is outperforming other partitioners in that regard as well. However, there could be issues with very large data for the edge cut quality, as can be seen in the results for the largest graph, where XtraPuLP showed a higher quality. \\
This leads to possible improvements that could be made to the algorithm. The edge balancing phase of the algorithm seems to be the best point for improvement, as during this phase the edge cut property is completely ignored. While this makes that phase really fast, the edge cut can suffer severely during this phase. Improving that phase would mean slowing it down, but looking at the excellent execution time of the algorithm, increasing the runtime for higher edge cut quality should be taken into consideration. \\
Concretely, an additional execution mode should be introduced for the algorithm. That way, the user could decide whether the very fast normal mode or a mode with increased edge cut quality should be run. As described in Section 4.3, the edge balancing phase only sorts vertices in partitions with too many edges by outdegree and relocates them randomly in descending order of outdegree. This additional mode could take the latest VP Score into consideration, which was communicated during the stabilization phase. That way, there should be a calculation introduced that looks at the vertices with the highest outdegree and checks if that vertex has a high VP Score. If that is the case, there should be a further check to see if it is beneficial to relocate other vertices with lower outdegrees instead. Additionally, instead of relocating the vertices to another partition with fewer edges at random, it should be checked for which of the partitions a relocation would be most beneficial for the edge cut. Overall, this could fix the issue of low edge cut quality seen in the experiment on the largest graph while increasing the runtime. 
\section{Conclusion}
All in all, the paper introduces a partitioner that performs really well on clusters with low memory resources. In that environment, multilevel-based partitioners struggle with or do not compute a solution at all, making this algorithm a great alternative. Also, the quick runtime is surely an argument for using this algorithm in all kinds of applications where a fast partitioner is needed. In addition to that, the algorithm is able to fulfill the given balancing restraints nearly perfectly. In the future, improvements to the edge cut of the algorithm could lead to even more use cases for this framework, and it could see usage in a variety of applications.
\bibliographystyle{plainnat}
\bibliography{references.bib}

\end{document}
