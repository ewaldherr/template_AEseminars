\documentclass[acmsmall,nonacm,screen,review]{acmart}
\newif\ifEnableExtend
%\EnableExtendtrue
\EnableExtendfalse

\usepackage{numprint}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\newcommand{\csch}[1]{{\color{red} Christian says: #1}}
\newcommand{\Is}       {:=}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{relsize}

\newtheorem{openproblem}{Open Problem}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\cov}{\term{cov}\xspace}
\newcommand{\term}[1]{\textsl{#1}}
\newcommand{\Comment}[1]{\textsl{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcopyright{none}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\title{Algorithm Engineering: Analysing the Impact of Graph Partitioning on Graph Neural Network Training}
\author{Eric Waldherr}
\email{ft278@stud.uni-heidelberg.de, Computer Science,
4231309}
\affiliation{%
  \institution{Heidelberg University}
  \streetaddress{Im Neuenheimer Feld 205}
  \city{Heidelberg}
  \state{Baden-Württemberg}
  \country{Germany}
  \postcode{69120}
}


\date{10th of July 2024}

\begin{document}

\begin{abstract}
Abstract here.
\end{abstract}
\maketitle

\section{Introduction}
Graphs have become a fundamental data structure in modern computer science over the last decades \cite{Junghanns2017}. There are many well-researched applications and algorithms that run on graphs. Furthermore, machine learning emerged as a significant field of study in numerous domains of computer science nowadays. Graph Neural Networks (GNNs) connect these two fields representing a form of machine learning that is capable of working with graph data. \\
The training methods of these structures are often highly memory-consuming. For this reason, many GNN frameworks support distributed training by utilize graph partitioning algorithms in order to use multiple processing units for the training steps. Moreover, it is well researched that the quality of a graph partitioning algorithm can have a considerable impact on the performance of the following algorithms run on the graph. \\
This leads to the question of how the quality of a partition can affect the runtime and quality of the subsequent training of the GNN. To answer this question, the Deep Graph Learn (DGL) framework \cite{DGL} was used as a basis. DGL employs the widely-used graph partitioner Metis \cite{Metis} for the distributed training of GNNs. In this report, the partitioner KaHIP \cite{KaHIP} is added to the DGL library to provide a comparison of these two partitioners and examining their impact on the performance and quality of the training. \\  
This report is structured as follows: First, the fundamental definitions regarding this topic are given in Section 2. Section 3 provides an insight into the used GNN framework and explains the most important training types of it. Furthermore, Section 4 explains in detail how KaHIP was implemented into DGL and other necessary changes made to the framework. The conducted experiment to compare the results between Metis and KaHIP is shown and analyzed in Section 5. Finally, the report closes with an overall picture of the conducted work in Section 6. 
\section{Preliminaries}
This chapter focuses on giving the most important definitions needed to understand the following chapters. Let $G=(V,E)$ be a graph, with $V$ as a set of vertices and $E \subseteq V \times V$ as a set of edges. An edge $(u,v)$ with $u,v \in V$ is called non-edge if $(u,v) \notin E$.
\subsection{Graph Partitioning}
The graph partitioning problem is a very fundamental problem in modern computer science. By initially partitioning a large graph first before running an algorithm on it, one can then run algorithms on the resulting partitions in parallel. This and the quality of the partition itself can have a significant impact on the runtime of an algorithm. \\ 
The input to the problem is a graph $G$ and $k\in \mathbb{N}$. To solve the problem, $G$ needs to be partitioned into $k$ partitions \hbox{$V_{1},...,V_{k}$ such that $V_{1}\cup...\cup V_{k} = V$} and $V_{i}\cap V_{j} = \emptyset\ \forall i,j\in \{1,...,k\}, i \neq j$. The primary objective of the partitioning algorithm is to minimize the edge cut, which can be defined as $\vert \{(u,v)\in E : u\in V_{i},\ v\in V_{j} \text{ and } i\neq j \}\vert$. This has the effect of reducing the amount of communication required between the various workers, which can significantly reduce the execution time of operations or algorithms run on the partitioned graph.  \\
In order to achieve the highest possible parallelization effectiveness, these partitions need to be balanced to distribute the resulting computation workload equally among the computation units. For that reason, the problem includes a vertex balancing restriction: $\forall i\in \{1,...,k\} : \vert V_{i}\vert \ \leq (1 + \delta) \lceil \frac{\vert V \vert }{k} \rceil $, with $\delta \in \mathbb{R}$ called vertex imbalance. Figure \ref{partition} shows a balanced partition. \\
In many cases, it is beneficial to balance the edges of the graph as well. This is done by assigning the nodes their outdegree plus one as weight. The outdegree of a node $v_{i}$ is defined as $deg(v_{i}) = \sum{(u,v)\in E : u = v_{i} \vee v = v_{i}}$. Figure \ref{weights} illustrates an example graph where each node $v$ is labeled with its respective weight, which is $deg(v) + 1$. \\
Furthermore, the distributed training of GNNs is not limited to the nodes of a partition itself. Each partition additionally comprises a set of halo nodes. A node $h \in V$ is called a halo node of a partition $V_{i}$ if $\exists (v,h) \in E \vee \exists (h,v) \in E$, for $v \in V_{i}, h \notin V_{i}$. In other words, a halo node of a partition shares an edge with one of the nodes in that partition without being part of the partition itself. These halo nodes are included in the training of each partition to increase the quality of the result. 
\subsection{Graph Neural Network Training}
How GNNs are trained in detail is a highly complex topic. In this instance, only the most important aspects are illustrated. The core of the training of GNNs are features. Each node $v \in V$ gets assigned a node feature $x_{v} \in \mathbb{R}^{d}$, with $d$ called feature size. \\
The training itself is made up of multiple steps, where the features of each node are updated based on the features of other nodes they are adjacent to. Step $t$ of this process can be explained as follows: $x_{v}^{(t)} = \phi(x_{v}^{(t-1)},\rho(\{x_{u}^{(t-1)} : (u,v) \in E \vee (v,u)\in E\}))$. $\rho$ is an aggregation function that uses the features of adjacent nodes as input, while $\phi$ is a function that updates the feature of a node based on its own features and the results of the $\rho$ function.\\ 
During the training, the GNN identifies patterns between the features of the nodes in connection other relevant node attributes depending on the training type. After the training is done a GNN is often used to predict certain attributes of nodes and edges making it a powerful tool in modern computer science.
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=3.5cm,height = 2 cm]{partition_example.png}
         \caption{Example balanced partition}
         \label{partition}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=2.8cm,height = 2cm]{node_weights.png}
         \caption{Example of node weights}
         \label{weights}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=3.2cm,height = 2cm]{halo_nodes.png}
        \caption{Illustration of halo nodes}
        \label{halo}
     \end{subfigure}
     \caption{Example of basic definitions}
     \label{definitions}
\end{figure}
\section{Related Work}
This chapter focuses on the DGL framework that was chosen as the GNN framework. The following section outlines the key features of DGL that were used to achieve the experimental results presented in Section 5. In general, DGL provides the possibility for the user to define their own GNN modules. Additionally, one can use the built-in GNN model frameworks. In the course of the experiments, GraphSAGE \cite{GraphSAGE} and GraphConv \cite{NC} were employed in particular. \\
To process and train on large graph datasets, DGL utilizes Metis per default. With this partitioner, the graphs can be partitioned first and then the training can be executed on each partition in parallel by the usage of multiple CPUs. This can achieve a significant speedup of the whole process. Now the main focus of this report is to investigate how the type and quality of the used graph partitioner can impact the runtime of the training process. For that, the graph partitioner KaHIP is utilized. Comparing the results of typical DGL training routines with the use of Metis and KaHIP can illustrate if the conventional graph partitioning quality metrics are applicable to GNNs. \\
In the course of this report, two types of DGL training are conducted for this comparison: node classification and link prediction. These are the most common tasks of DGL and can be explained as follows: semi-supervised node classification, as defined in detail by Kipf et al. \cite{NC}, is the task of predicting the category of each node in the input graph. For that purpose, the nodes of the graph are distributed into three subsets: validation, training and testing. \\  
\begin{figure}[bt!]
    \centering
    \includegraphics[width=9.5cm,height = 3.5cm]{gnn.png}
    \caption{Illustration of GNN training process based on node features}
    \label{learning}
\end{figure}
First, the GNN uses the training set to identify patterns between the features and labels of the nodes within that set. Subsequently, it predicts the labels of the nodes of the validation set based on their features. During this phase, the GNN adjusts its parameters depending on the accuracy of the predictions made. This includes updating the features of each node based on their adjacent nodes, illustrated in Figure \ref{learning}. Finally, the GNN predicts the label of each node of the test set, once more using the feature of the nodes. This process is then repeated for a certain amount of epochs. As the number of epochs increases, the GNN's accuracy improves, resulting in more accurate predictions overall. \\
Link prediction is the process of predicting whether there is an edge between two nodes in a graph or not. This task does not require additional sample data, such as the labels and features of the node classification task. In contrast, the input graph itself provides the sample data. A part of the existing edges in the graph are taken as positive examples. Similar, a portion of the non-edges are taken as negative examples. This data is used to minimize the prediction error of the GNN over a certain amount of epochs. Ultimately, the GNN is capable of predicting the existence of an edge or a non-edge between two nodes for all the potential edges in the graph that were not part of the sample data.
\section{Implementation}

This section illustrates how KaHIP was integrated into the DGL framework and what changes needed to be made to DGL to achieve that.
\subsection{Integration of KaHIP to DGL}
First of all, KaHIP was added to the third-party frameworks of DGL. After that, it was included in the building process, ensuring it gets compiled correctly. In DGL, the majority of complex functionalities are realized as follows: A file is created for each functionality, wherein a function is defined that can be called at a later stage outside of that individual file. Therefore, a new function was created that calls the main partitioner of KaHIP, named kaffpa. The kaffpa function requires all the connectivity information of the graph in order to work properly. In particular, the kaffpa function requires the amount of nodes in the graph as well as the source and destination node for each edge in the graph. This information was extracted by the respective DGL graph class utilizing the predefined methods of that class. Additionally, kaffpa must be configured. In essence, one of the six available modes of the function must be selected. The FAST mode was selected by default as a preliminary step. At that point in time, this was sufficient, as the objective was to initiate the operation of KaHIP. Subsequently, the detailed configurations were implemented at a later stage.\\
After this first crucial step was done, a crucial problem occurred. All the essential information of the graph is given to kaffpa in four arrays of integers. The conflict here was that DGL saves this information in 64-bit arrays, whereas the kaffpa function expects 32-bit arrays. To resolve this conflict, it was necessary to modify either DGL or KaHIP to accept the integer size of the other component, respectively. As modifying DGL would result in changing significantly more code, KaHIP was adjusted instead. This modification involved changing the function signatures of 21 functions inside of KaHIP, distributed across 10 files, as well as modifying the type definitions in its definition header to accept 64-bit integers for the ids of nodes and edges as well as their weights. These changes were successful in achieving their objective, whereby KaHIP could now be called successfully within DGL using 64-bit edges and nodes. \\
The next step was to include this new functionality in the corresponding partition utilities file. In this instance, an additional setup and a post-processing step are necessary before and after the actual partitioning function is called. This partitioning function was implemented as described above. The initial step is to transform the graph into a bidirectional graph. This means that for each $(u,v) \in E$ the corresponding backward edge $(v,u)$ is added to $E$. This is a standard intial step that is done before partitioning in DGL and is also done before every partition call of Metis. This process is illustrated in Figure \ref{bidirecting}. Then, after the partitioning process concludes, the halo nodes of each partition are identified. Subsequently, these nodes are added as to the partitions' halo nodes, respectively. This step in the implementation opened the possibility to call a KaHIP partition in DGL and then store the result and continue working on the different partitions at a later stage. \\
Even though a KaHIP partition could be conducted at this stage of development, the feature to actually utilize the partition in a distributed parallel context remained to be implemented. A function for this purpose had already been provided by DGL, but thus far only for a Metis partition call. The default behaviour of the function was to call a Metis partition. Each partition of the result was then stored in a destination directory in its own file. These individual partitions can then be loaded independently and then used by different computation units in parallel. The option of selecting either Metis or KaHIP when calling this function was enabled by adding an additional parameter that determines which partitioner to call. This concluded the integration of KaHIP into the DGL framework and each of the two partitioners could be called by the user.
\subsection{Additional Changes for Experiment}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=3.45cm,height = 2 cm]{directed_graph.png}
         \caption{Original graph}
         \label{uni}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=3.45cm,height = 2cm]{bidirectional_graph.png}
         \caption{Graph after adding backward edges}
         \label{bi}
     \end{subfigure}
     \caption{Process of bidirecting a graph}
     \label{bidirecting}
\end{figure}
Further features were added to the partitioning process of DGL to provide a more thorough analysis of the conducted experiments. Edge balancing is a crucial quality metric for the graph partitioning process. Accordingly, the subsequent objective was to add $deg(v) + 1$ as a weight to each node $n \in V$. This was accomplished by including an additional setup step before calling a partition, where the information of the outdegree was extracted from the graph. This was achieved through a function, provided by DGL, that returns a tensor. Each entry of that tensor represents the outdegree of a node in the graph. Then the value 1 was required to be added to each of those entries. This tensor could then be utilized by handing it to the partitioners by an additional parameter. As this method of edge balancing is a common one, the partitioners are able to accept these arrays and are able to return a partition that is balanced based on that array. This feature was introduced to investigate even further examine the influence of conventional partition quality metrics on the performance of GNNs. Moreover, a parameter was added to activate and deactivate this feature for different partitioning executions.  \\
KaHIP provides six distinct execution modes in total. The available modes are FAST, ECO, STRONG, FASTSOCIAL, ECOSOCIAL and STRONGSOCIAL.  At the current implementation stage, only the FAST mode was available, as previously stated in Section 4.1. To be able to choose from these modes, an additional parameter was introduced to all of the functions that include the KaHIP partitioning call. This allows for an insight into the impact of the different modes on different types of graphs in the experiment.

\section{Experiment}
\begin{table}[bt!]
\centering
\begin{tabular}{ ccc }
 \centering
  & Node classification & Link prediction \\ 
 \hline
 Distribution & single & fat \\
 CPUs & \multicolumn{2}{c}{Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz} \\ 
 \# of CPUs & 32 & 32 \\ 
 Memory & 16GB & 356GB \\
\end{tabular}
\caption{Specifications of the used cluster}
\label{hw}
\end{table}
\begin{table}[bt!]
\centering
\begin{tabular}{ cccccc }
 \centering
 Graph & $|V|$ & $|E|$ & node feature size & \# of node categories \\ 
 \hline
 Questions \cite{TQ} & 49k & 307k & 301 & 2 \\ 
 Flickr \cite{Flickr} & 89k & 900k & 500 & 7\\ 
 Tolokers \cite{TQ,Tolokers} & 12k & 1M & 10 & 2 \\

\end{tabular}
\caption{Graphs used for the experiment}
\label{graphs}
\end{table}
This section provides a comprehensive description of the experimental environment. Afterwards, the results of the experiments are presented and evaluated. This includes a comparison of Metis and KaHIP performing a graph partition followed by the same subsequent GNN training. The execution times of the training are compared for different partitioning sizes and vertex weights between Metis and all the available KaHIP execution modes. 
\subsection{Experimental Environment}
The experiment was conducted on the BwUniCluster2.0 computing cluster. In the case of node classification, the single distribution was employed, whereas for link prediction, the fat distribution of the cluster was utilised. The configuration of the cluster employed in the experiment is detailed in Table \ref{hw}.\\
Three graphs were used for the experiment in total. The sizes, feature specifications, and number of distinct node categories for each graph are presented in Table \ref{graphs}. The original Tolokers and Questions graph only have half of the edges shown in Table \ref{graphs}. DGL already included these two graphs into their example graph data sets and added all the backward edge for each edge on the original graph.\\
The two training types were executed as follows: The initial step is to partition the graph. Subsequently, the actual training for each partition is conducted in parallel using PyTorch \cite{PyTorch} and 100 epochs. The PyTorch library provides functions to execute GNN training in parallel, through the creation of a new process for each partition, which were utilized for the experiment. The training was repeated three times for each instance. Subsequently, the mean of the three runs was determined and used for the diagrams of the following subchapters. Each graph was partitioned twice using the same configurations (for node classification and for link prediction). The average edge cut produced out of these two partition processes was taken for the edge cut results, shown in Figure \ref{cut}.
\subsection{Evaluation of Node Classification Results}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{nc_in_4.png}
         \caption{Partitioning time included}
         \label{nc_in_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{nc_ex_4.png}
         \caption{Partitioning time excluded}
         \label{nc_ex_4}
     \end{subfigure}
     \caption{Execution time of node classification using 4 partitions}
     \label{nc_results}
\end{figure}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{edge-cut_4.png}
         \caption{Edge cut for 4 partitions}
         \label{cut_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{edge-cut_8.png}
         \caption{Edge cut for 8 partitions}
         \label{cut_8}
     \end{subfigure}
     \caption{Produced edge cut of the algorithms}
     \label{cut}
\end{figure}
In this phase of the experiment, the graphs were primarily partitioned into four parts. The produced edge cut is listed in Figure \ref{cut_4}. In the majority of cases, the KaHIP modes FAST and ECO resulted in the highest edge cut. Then, Metis and the modes FASTSOCIAL and ECOSOCIAL follow. The best edge cut was achieved by the STRONG and STRONGSOCIAL mode of KaHIP. \\ 
However, these low edge cuts result in very high partitioning times leading to poor execution times as illustrated in Figure \ref{nc_in_4}. This figure shows the execution times for each algorithm and for each graph with and without vertex weights. Additionally, this illustration demonstrates that overall the execution time of Metis and the FASTSOCIAL mode of KaHIP performed the best.  \\
Figure \ref{nc_ex_4} presents the execution times as Figure \ref{nc_in_4}, but without including the time required by the partitioning algorithms to complete. This diagram illustrates the actual impact of the graph partitioning on the GNN training time. The results demonstrate that the training process following a Metis partition is outperformed by the STRONG, FASTSOCIAL, ECOSOCIAL and STRONGSOCIAL mode of KaHIP in the majority of cases. These four modes achieved superior edge cut quality overall in comparison to Metis. This implies, that the edge cut metric has a significant impact on GNN node classification training. In the experiment 100 epochs were conducted for the training. Generally, the accuracy of GNNs is increasing with an increasing amount of epochs. Considering this fact makes clear, that the impact of graph partitioning on GNN training time increases with more epochs of training. \\
A closer examination of the training times and the resulting edge cut reveals that the FASTSOCIAL and ECOSOCIAL modes perform best in most instances. However, the edge cut of the STRONGSOCIAL and STRONG mode is better. This suggests that there may be additional partition quality metrics that are specific to GNNs.\\
Another aspect Figure \ref{lp_ex_4} illustrates is, that the use of vertex weights to achieve a balanced partitioning has a considerable positive impact on the training of GNNs. In the majority of instances, including vertex weights decreased the training time of the GNN. This marks another quality metric of graph partitioning that has a positive effect on GNN training.\\
Figure \ref{nc_comp} reports the execution time for node classification on the Flickr graph for varying numbers of partitions, excluding the time required for graph partitioning. As the number of partitions increases, the execution time is reduced. This illustrates that an increase in the number of partitions leads to a greater degree of parallelisation and highlights the importance of graph partitioning in general. This speedup is expected to be even higher when executing node classification tasks on larger graph datasets. Additionally, it is notable that for four partitions only the STRONGSOCIAL mode of KaHIP produces superior training times in comparison to Metis. Increasing the amount of partitions to 16, leads to three additional KaHIP modes outperforming Metis.\\
Overall, the results of the node classification training demonstrate that this task is affected positively by common partition quality metrics. However, the optimal choice for a partitioning algorithm seems to be dependent on a number of factors, including the number of partitions and the size of the graph.
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=4.1cm,height = 3 cm]{nc_comp.png}
         \caption{Node classification runtime on the Flickr graph}
         \label{nc_comp}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=4.1cm,height = 3cm]{lp_comp.png}
         \caption{Link prediction runtime on the Tolokers graph}
         \label{lp_comp}
     \end{subfigure}
     \caption{GNN training time using different numbers of partitions}
     \label{parts}
\end{figure}
\subsection{Evaluation of Link Prediction Results}
For this task the main experiments were conducted using eight partitions. The resulting edge cuts are presented in Figure \ref{cut_8}. The algorithms performed similar to the edge cut results observed in Figure \ref{cut_4}. The STRONG and STRONGSOCIAL modes of KaHIP produce the best edge cut, while the FAST and ECO modes exhibited the least favourable performance in this regard. \\
Looking at the execution time of the whole procedure, illustrated in Figure \ref{lp_in_4}, the relatively high partitioning time of the STRONG and the STRONGSOCIAL modes resulted in suboptimal execution times. The most effective algorithms were Metis and the FASTSOCIAL mode. \\
These two algorithms also have the greatest impact on training time. This can be observed in Figure \ref{lp_ex_4}. Furthermore, the impact of the edge cut on the training times is smaller for the link prediction task than observed for the node classification. In some instances, the STRONG and STRONGSOCIAL modes perform poorly in some instances despite their excellent edge cut. Moreover, the performance of each partitioning algorithm differs significantly for different instances. Therefore, it is essential to select a graph partitioner carefully, depending on the exact use case. \\
The addition of node weights has a beneficial effect on training outcomes, with the greatest impact observed in the case of the Questions graph in Figure \ref{lp_ex_4}. In most other instances, node weights resulted in a positive impact as well. Given that link prediction is an edge-based GNN training type, this behaviour was to be expected. As this can be observed for both training types, the edge balance quality metric appears to be a fundamental aspect of for GNN training. \\
The link prediction was conducted for varying numbers of partitions on the Tolokers graph. The resulting GNN training times are illustrated in Figure \ref{lp_comp}. In the majority of instances, increasing the amount of partitions led to a reduction in the time taken to complete the training process. The number of partitions influences the performance of the training process significantly. This can be observed in the unweighted node case. In that case, using four partitions result in the FAST and ECO modes of KaHIP performing poorly, but for 16 partitions they perform the best. This aspect reinforces the assumption that the partitioner should be selected based on the specific use case.\\
Overall, this experiment demonstrate that the choice of graph partitioning algorithm has a significant impact on the performance of GNN training. Furthermore, for the link prediction tasks, the FASTSOCIAL mode of KaHIP is the optimal choice in the majority of instances.
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{lp_in_8.png}
         \caption{Partitioning time included}
         \label{lp_in_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{lp_ex_8.png}
         \caption{Partitioning time excluded}
         \label{lp_ex_4}
     \end{subfigure}
     \caption{Execution time of link prediction using 8 partitions}
     \label{lp_results}
\end{figure}
\section{Conclusion}
In this study, the high-quality partitioner KaHIP was introduced to the DGL framework. The experiment conducted demonstrates that distributed GNN training is significantly impacted by the enforced graph partitioner. The use of outdegree as a weighting factor, thus balancing the edges for the partitions, has been found to improve subsequent training time. Moreover, it was found that fundamental partition quality metrics influence GNNs in a positive manner.\\
However, in some instances, a high-quality partition did not result in a superior execution time for the training. This leads to the assumption that there are more metrics that affect the training of GNNs, which differ from common partitioning quality metrics. Further investigation into this possibility and the identification of more accurate metrics for GNNs could lead to the development of graph partitioners based on these metrics. This could potentially lead to a significant acceleration of GNN training in the future.\\
Furthermore, the experiment made clear, that different types of partitioners can be advantageous, depending on the specific use case of GNN training. Consequently, future developments in GNN frameworks like DGL should include the introduction of additional graph partitioners. This would allow the users to select the partitioner based on their GNN training leading to the optimal execution times.
\section{Acknowledgments}
The author would like to express gratitude to Adil Chhabra, Ernestine Grossmann, Kenneth Langedal and Christian Schulz for their supervision and support of the project. Furthermore, the author acknowledges the support provided by the state of Baden-Württemberg through BwHPC.
\bibliographystyle{plainnat}
\bibliography{references.bib}

\end{document}
