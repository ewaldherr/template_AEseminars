\documentclass[acmsmall,nonacm,screen,review]{acmart}
\newif\ifEnableExtend
%\EnableExtendtrue
\EnableExtendfalse

\usepackage{numprint}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\newcommand{\csch}[1]{{\color{red} Christian says: #1}}
\newcommand{\Is}       {:=}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{relsize}

\newtheorem{openproblem}{Open Problem}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\cov}{\term{cov}\xspace}
\newcommand{\term}[1]{\textsl{#1}}
\newcommand{\Comment}[1]{\textsl{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcopyright{none}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\title{Algorithm Engineering: Analysing the Impact of Graph Partitioning on Graph Neural Network Training}
\author{Eric Waldherr}
\email{ft278@stud.uni-heidelberg.de, Computer Science,
4231309}
\affiliation{%
  \institution{Heidelberg University}
  \streetaddress{Im Neuenheimer Feld 205}
  \city{Heidelberg}
  \state{Baden-WÃ¼rttemberg}
  \country{Germany}
  \postcode{69120}
}


\date{10th of July 2024}

\begin{document}

\begin{abstract}
Abstract here.
\end{abstract}
\maketitle

\section{Introduction}
Graphs have become a fundamental data structure in modern computer science over the last decades \cite{Junghanns2017}. There are many well-researched applications and algorithms that run on graphs. Furthermore, machine learning emerged recently and is a relevant topic in countless fields of computer science nowadays. Graph Neural Networks (GNNs) connect these two fields as a form of machine learning that is able to work with graph data. \\
The training methods of these structures are often highly memory-consuming. For that reason, many GNN frameworks support distributed training by using graph partitioning algorithms in order to use multiple processing units for the training steps. Furthermore, it is well researched that the quality of a graph partitioning algorithm can have a significant impact on the performance of the following algorithms run on the graph. \\
This leads to the question of how the quality of a partition can affect the runtime and quality of the subsequent training of the GNN. To answer this question, the Deep Graph Learn (DGL) framework \cite{DGL} was used as a basis. DGL uses the popular graph partitioner Metis \cite{Metis} for distributed training of GNNs. In this report, the partitioner KaHIP \cite{KaHIP} is added to the DGL library to provide a comparison of these two partitioners and see how they impact the performance and quality of the training. \\  
This report is structured as follows: First, the fundamental definitions regarding this topic are given in Section 2. Section 3 gives an insight into the used GNN framework and explains the most important training types of it. Furthermore, Section 4 explains in detail how KaHIP was implemented into DGL and other necessary changes made to the framework. The conducted experiment to compare the results between Metis and KaHIP is shown and analyzed in Section 5. Finally, the report closes with an overall picture of the conducted work in Section 6. 
\section{Preliminaries}
This chapter focuses on giving the most important definitions needed to understand the following chapters. Let $G=(V,E)$ be a graph, with $V$ as a set of vertices and $E \subseteq V \times V$ as a set of edges. An edge $(u,v)$ with $u,v \in V$ is called non-edge if $(u,v) \notin E$.
\subsection{Graph Partitioning}
The graph partitioning problem is a very fundamental problem in modern computer science as graphs. By partitioning a huge graph first before running an algorithm on it, one can run algorithms on the resulting partitions in parallel. This and the quality of the partition itself can have a significant impact on the runtime of an algorithm. \\ 
The input of the problem is a graph $G = (V,E)$ and $k\in \mathbb{N}$. To solve the problem, $G$ needs to be partitioned into $k$ partitions \hbox{$V_{1},...,V_{k}$ such that $V_{1}\cup...\cup V_{k} = V$} and $V_{i}\cap V_{j} = \emptyset\ \forall i,j\in \{1,...,k\}, i \neq j$. The main goal of the partitioning algorithm is to minimize the edge cut, which can be defined as $\vert \{(u,v)\in E : u\in V_{i},\ v\in V_{j} \text{ and } i\neq j \}\vert$. This reduces the amount of communication workload needed between the different workers and can significantly reduce the execution time of operations or algorithms run on the partitioned graph.  \\
To achieve the highest possible parallelization effectiveness, these partitions need to be balanced to distribute the resulting computation workload equally among the computation units. For that reason, the problem includes a vertex balancing restriction: $\forall i\in \{1,...,k\} : \vert V_{i}\vert \ \leq (1 + \delta) \lceil \frac{\vert V \vert }{k} \rceil $, with $\delta \in \mathbb{R}$ called vertex imbalance. Figure \ref{partition} shows a balanced partition. \\
Often, it can be beneficial to balance the edges of the graph as well. This is done by assigning the nodes their outdegree + 1 as weight. The outdegree of a node $v_{i}$ is defined as $deg(v_{i}) = \sum{(u,v)\in E : u = v_{i} \vee v = v_{i}}$. Figure \ref{weights} shows an example graph where each node $v$ is labeled with its respective weight, based on $deg(v) + 1$. \\
Furthermore, the distributed training of GNNs is not limited to the nodes of a partition itself. Each partition additionally has a set of halo nodes. A node $h \in V$ is called a halo node of a partition $V_{i}$ if $\exists (v,h) \in E \vee \exists (h,v) \in E$, for $v \in V_{i}, h \notin V_{i}$. In other words, a halo node of a partition shares an edge with one of the nodes in that partition without being part of the partition itself. These halo nodes are included in the training of each partition to increase the quality of the result. 
\subsection{Graph Neural Network Training}

\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=3.5cm,height = 2 cm]{partition_example.png}
         \caption{Example balanced partition}
         \label{partition}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=2.8cm,height = 2cm]{node_weights.png}
         \caption{Example of node weights}
         \label{weights}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=3.2cm,height = 2cm]{halo_nodes.png}
        \caption{Illustration of halo nodes}
        \label{halo}
     \end{subfigure}
     \caption{Example of basic definitions}
     \label{definitions}
\end{figure}
\section{Related Work}
This chapter focuses on the DGl framework that was chosen as the GNN framework. In the following, the most important features of DGL are explained that were used to achieve the experimental results of Section 5. In general, DGL provides the possibility for the user to define their own GNN modules. Additionally, one can use the built-in GNN model frameworks. In the course of the experiments, GraphSAGE \cite{GraphSAGE} and GraphConv(?) \cite{NC} were used in particular. \\
To process and train on huge graph data sets, DGL utilizes Metis per default. With this partitioner, the graphs can be partitioned first and then the training can be run on every partition in parallel by the use of multiple CPUs. This can achieve a significant speedup of the whole process. Now the main focus of this report is to investigate how the type and quality of the used graph partitioner can impact the runtime of the training process. For that, the graph partitioner KaHIP is used. Comparing the results of typical DGL training routines with using Metis and KaHIP can illustrate if the common graph partitioning quality measures also apply for GNNs. \\
In the course of this report, two types of DGL training are run for this comparison: node classification and link prediction. These are the most common tasks of DGL and can be explained as follows: Semi-supervised node classification, as defined in detail by Kipf et al. \cite{NC}, is the task of predicting the category of each node in the input graph. For that, the nodes of the graph are distributed into three subsets: validation, training and testing. \\ First, the GNN uses the training set to search for patterns between the features and labels of the nodes in that set. Then, it predicts the labels of the nodes of the validation set based on their features. During that phase, the GNN adjusts its parameters depending on the accuracy of the predictions made. Finally, the GNN predicts the label of each node of the test set, again using the feature of the nodes. This process is then repeated for a certain amount of epochs. The GNN improves its accuracy over the epochs, resulting in generally accurate predictions in the end. \\
Link prediction is the task of predicting whether there is an edge between two nodes in a graph or not. This task does not depend on additional sample data like the labels and features of the node classification task. Instead, the input graph itself provides the sample data. A part of the existing edges of the graph are taken as positive examples. Similar, a portion of the non-edges are taken as negative examples. This data is used to minimize the prediction error of the GNN over a certain amount of epochs. Finally, the GNN is able to evaluate if there is an edge or a non-edge between two nodes for all the potential edges in the graph that were not part of the sample data.
\section{Implementation}
\begin{figure}[b]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=3.45cm,height = 2 cm]{directed_graph.png}
         \caption{Original graph}
         \label{uni}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=3.45cm,height = 2cm]{bidirectional_graph.png}
         \caption{Graph after adding backward edges}
         \label{bi}
     \end{subfigure}
     \caption{Process of bidirection a graph}
     \label{bidirecting}
\end{figure}
This section illustrates how KaHIP was integrated into the DGL framework and what changes needed to be made to DGL to achieve that.
\subsection{Integration of KaHIP to DGL}
First of all, KaHIP was added to the third-party frameworks of DGL. After that, it was included in the building process, ensuring it gets compiled correctly. In DGL, most more complex functionalities are realized as follows: There exists a file for each functionality where a function is defined that can be called later outside of that individual file. In that way, a new function was created that calls the main partitioner of KaHIP, named kaffpa. The kaffpa function needs all the connectivity information of the graph in order to work properly. In detail, it needs the amount of nodes in the graph as well as the source and destiny node for every edge in the graph. This information was extracted by the respective DGL graph class using the already defined methods of that class. Additionally, kaffpa needs to be configured. Mainly, one of the six modes of that function needs to be chosen. For the start, the FAST mode was chosen by default. At that point in time, this was sufficient, as the focus lied on getting KaHIP running. Then, the detailed configurations followed at a later stage.\\
After this first crucial step was done, a crucial problem occurred. All the main information of the graph is given to kaffpa in four arrays of integers. The conflict here was that DGL saves this information in 64-bit arrays while the kaffpa function expects 32-bit arrays. To resolve this conflict, either DGL or KaHIP needed to be changed to accept the integer size of the other component, respectively. As changing DGL would result in changing significantly more code, KaHIP was changed instead. This change consisted of changing the function signatures of 21 functions inside of KaHIP distributed over 10 files as well as changing type definitions in its definition header to accept 64-bit integers for the ids of nodes and edges as well as their weights. These changes achieved their goal and KaHIP could now be called successfully inside of DGL using 64-bit edges and nodes. \\
The next step was to include this new functionality in the corresponding partition utilities file. Here, an additional setup and a cleanup step are necessary before and after the actual partitioning function, which was implemented as described above, is called. First, the graph needs to be transformed to a bidirectional graph. This means that for every $(u,v) \in E$ the corresponding backward edge $(v,u)$ is added to $E$. This is a default step that is done before partitioning in DGL and is also done before every partition call of Metis. This process is illustrated in Figure \ref{bidirecting}. Then, after the partition concludes, the halo nodes of each partition are determined. These nodes are then added as to the partitions' halo nodes, respectively. This step in the implementation opened the possibility to call a KaHIP partition in DGL and then store the result and continue working on the different partitions at a later stage. \\
Even though a KaHIP partition could be conducted at this stage of development, the feature to actually utilize the partition in a distributed parallel context was still to be implemented. A function was already provided by DGL, but so far only for a Metis partition call. The function called a Metis partition per default. Every partition of the result was then stored under a destination directory in its own file. These individual partitions can then be loaded independently and then used by different computation units in parallel. The possibility to choose from Metis and KaHIP when calling this function was enabled by adding an additional parameter that determines which partitioner to call. This concluded the integration of KaHIP into the DGL framework and each of the two partitioners could be called by the user.
\subsection{Addtitional Changes for Experiment}
Further features were added to the partitioning process of DGL to provide a more thorough analysis in the conducted experiments. Edge balancing is a crucial quality measure of a graph partition. For that reason, the next goal was to add $deg(v) + 1$ as a weight to each node $n \in V$. This was achieved by adding an additional setup step before calling a partition, where the information of the outdegree was extracted from the graph. This could be done through a function, provided by DGL, that returns a tensor. Each entry of that tensor represents the outdegree of a node in the graph. Then the value 1 needed to be added to each of those entries. This tensor could then be utilized by handing it to the partitioners by an additional parameter. As this method of edge balancing is very common, the partitioners accept these arrays and are able to return a partition that is balanced based on that array. This feature was added to investigate even further how traditional partition quality measures impact the performance of GNNs. \\
KaHIP provides six different execution modes in total: FAST, ECO, STRONG, FASTSOCIAL, ECOSOCIAL and STRONGSOCIAL. So far, only the FAST mode was available at the current implementation stage, as mentioned in Section 4.1. To be able to choose from these modes, an additional parameter was added to all of the functions that include the KaHIP partitioning call. This would create an insight into the impact of the different modes on different types of graphs for the experiment.

\section{Experiment}
\subsection{Experimental Environment}
\subsection{Evaluation}
\begin{table}[bt!]
\centering
\begin{tabular}{ cccc }
 \centering
 Graph & Type & $|V|$ & $|E|$  \\ 
 \hline
 Hollywood-2011 & Colla. & 2M & 229M \\ 
 Enwiki-2021 & Wiki & 6M & 150M \\ 
 Eu-2015-tpd & Web & 7M & 166M 

\end{tabular}
\label{graphs}
\caption{Graphs used for the experiment}
\end{table}
\section{Conclusion}

\bibliographystyle{plainnat}
\bibliography{references.bib}

\end{document}
