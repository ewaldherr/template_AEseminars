\documentclass[acmsmall,nonacm,screen,review]{acmart}
\newif\ifEnableExtend
%\EnableExtendtrue
\EnableExtendfalse

\usepackage{numprint}
\usepackage{subcaption}
\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\newcommand{\csch}[1]{{\color{red} Christian says: #1}}
\newcommand{\Is}       {:=}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{relsize}

\newtheorem{openproblem}{Open Problem}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\cov}{\term{cov}\xspace}
\newcommand{\term}[1]{\textsl{#1}}
\newcommand{\Comment}[1]{\textsl{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcopyright{none}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\title{Algorithm Engineering: Analysing the Impact of Graph Partitioning on Graph Neural Network Training}
\author{Eric Waldherr}
\email{ft278@stud.uni-heidelberg.de, Computer Science,
4231309}
\affiliation{%
  \institution{Heidelberg University}
  \streetaddress{Im Neuenheimer Feld 205}
  \city{Heidelberg}
  \state{Baden-Württemberg}
  \country{Germany}
  \postcode{69120}
}


\date{10th of July 2024}

\begin{document}

\begin{abstract}
Abstract here.
\end{abstract}
\maketitle

\section{Introduction}
Graphs have become a fundamental data structure in modern computer science over the last decades \cite{Junghanns2017}. There are many well-researched applications and algorithms that run on graphs. Furthermore, machine learning emerged as a significant field of study in numerous domains of computer science nowadays. Graph Neural Networks (GNNs) connect these two fields representing a form of machine learning that is capable of working with graph data. \\
The training methods of these structures are often highly memory-consuming. For this reason, many GNN frameworks support distributed training by utilize graph partitioning algorithms in order to use multiple processing units for the training steps. Moreover, it is well researched that the quality of a graph partitioning algorithm can have a considerable impact on the performance of the following algorithms run on the graph. \\
This leads to the question of how the quality of a partition can affect the runtime and quality of the subsequent training of the GNN. To answer this question, the Deep Graph Learn (DGL) framework \cite{DGL} was used as a basis. DGL employs the widely-used graph partitioner Metis \cite{Metis} for the distributed training of GNNs. In this report, the partitioner KaHIP \cite{KaHIP} is added to the DGL library to provide a comparison of these two partitioners and examining their impact on the performance and quality of the training. \\  
This report is structured as follows: First, the fundamental definitions regarding this topic are given in Section 2. Section 3 provides an insight into the used GNN framework and explains the most important training types of it. Furthermore, Section 4 explains in detail how KaHIP was implemented into DGL and other necessary changes made to the framework. The conducted experiment to compare the results between Metis and KaHIP is shown and analyzed in Section 5. Finally, the report closes with an overall picture of the conducted work in Section 6. 
\section{Preliminaries}
This chapter focuses on giving the most important definitions needed to understand the following chapters. Let $G=(V,E)$ be a graph, with $V$ as a set of vertices and $E \subseteq V \times V$ as a set of edges. An edge $(u,v)$ with $u,v \in V$ is called non-edge if $(u,v) \notin E$.
\subsection{Graph Partitioning}
The graph partitioning problem is a very fundamental problem in modern computer science. By initially partitioning a large graph first before running an algorithm on it, one can then run algorithms on the resulting partitions in parallel. This and the quality of the partition itself can have a significant impact on the runtime of an algorithm. \\ 
The input to the problem is a graph $G$ and $k\in \mathbb{N}$. To solve the problem, $G$ needs to be partitioned into $k$ partitions \hbox{$V_{1},...,V_{k}$ such that $V_{1}\cup...\cup V_{k} = V$} and $V_{i}\cap V_{j} = \emptyset\ \forall i,j\in \{1,...,k\}, i \neq j$. The primary objective of the partitioning algorithm is to minimize the edge cut, which can be defined as $\vert \{(u,v)\in E : u\in V_{i},\ v\in V_{j} \text{ and } i\neq j \}\vert$. This has the effect of reducing the amount of communication required between the various workers, which can significantly reduce the execution time of operations or algorithms run on the partitioned graph.  \\
In order to achieve the highest possible parallelization effectiveness, these partitions need to be balanced to distribute the resulting computation workload equally among the computation units. For that reason, the problem includes a vertex balancing restriction: $\forall i\in \{1,...,k\} : \vert V_{i}\vert \ \leq (1 + \delta) \lceil \frac{\vert V \vert }{k} \rceil $, with $\delta \in \mathbb{R}$ called vertex imbalance. Figure \ref{partition} shows a balanced partition. \\
In many cases, it is beneficial to balance the edges of the graph as well. This is done by assigning the nodes their outdegree plus one as weight. The outdegree of a node $v_{i}$ is defined as $deg(v_{i}) = \sum{(u,v)\in E : u = v_{i} \vee v = v_{i}}$. Figure \ref{weights} illustrates an example graph where each node $v$ is labeled with its respective weight, which is $deg(v) + 1$. \\
Furthermore, the distributed training of GNNs is not limited to the nodes of a partition itself. Each partition additionally comprises a set of halo nodes. A node $h \in V$ is called a halo node of a partition $V_{i}$ if $\exists (v,h) \in E \vee \exists (h,v) \in E$, for $v \in V_{i}, h \notin V_{i}$. In other words, a halo node of a partition shares an edge with one of the nodes in that partition without being part of the partition itself. These halo nodes are included in the training of each partition to increase the quality of the result. 
\subsection{Graph Neural Network Training}
How GNNs are trained in detail is a highly complex topic. In this instance, only the most important aspects are illustrated. The core of the training of GNNs are features. Each node $v \in V$ gets assigned a node feature $x_{v} \in \mathbb{R}^{d}$, with $d$ called feature size. \\
The training itself is made up of multiple steps, where the features of each node are updated based on the features of other nodes they are adjacent to. Step $t$ of this process can be explained as follows: $x_{v}^{(t)} = \phi(x_{v}^{(t-1)},\rho(\{x_{u}^{(t-1)} : (u,v) \in E \vee (v,u)\in E\}))$. $\rho$ is an aggregation function that uses the features of adjacent nodes as input, while $\phi$ is a function that updates the feature of a node based on its own features and the results of the $\rho$ function.\\ 
During the training, the GNN identifies patterns between the features of the nodes in connection other relevant node attributes depending on the training type. After the training is done a GNN is often used to predict certain attributes of nodes and edges making it a powerful tool in modern computer science.
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=3.5cm,height = 2 cm]{partition_example.png}
         \caption{Example balanced partition}
         \label{partition}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.3\textwidth}
         \centering
         \includegraphics[width=2.8cm,height = 2cm]{node_weights.png}
         \caption{Example of node weights}
         \label{weights}
     \end{subfigure}
     \begin{subfigure}[b]{0.3\textwidth}
        \centering
        \includegraphics[width=3.2cm,height = 2cm]{halo_nodes.png}
        \caption{Illustration of halo nodes}
        \label{halo}
     \end{subfigure}
     \caption{Example of basic definitions}
     \label{definitions}
\end{figure}
\section{Related Work}
This chapter focuses on the DGL framework that was chosen as the GNN framework. The following section outlines the key features of DGL that were used to achieve the experimental results presented in Section 5. In general, DGL provides the possibility for the user to define their own GNN modules. Additionally, one can use the built-in GNN model frameworks. In the course of the experiments, GraphSAGE \cite{GraphSAGE} and GraphConv(?) \cite{NC} were employed in particular. \\
To process and train on large graph datasets, DGL utilizes Metis per default. With this partitioner, the graphs can be partitioned first and then the training can be executed on each partition in parallel by the usage of multiple CPUs. This can achieve a significant speedup of the whole process. Now the main focus of this report is to investigate how the type and quality of the used graph partitioner can impact the runtime of the training process. For that, the graph partitioner KaHIP is utilized. Comparing the results of typical DGL training routines with the use of Metis and KaHIP can illustrate if the conventional graph partitioning quality metrics are applicable to GNNs. \\
In the course of this report, two types of DGL training are conducted for this comparison: node classification and link prediction. These are the most common tasks of DGL and can be explained as follows: semi-supervised node classification, as defined in detail by Kipf et al. \cite{NC}, is the task of predicting the category of each node in the input graph. For that purpose, the nodes of the graph are distributed into three subsets: validation, training and testing. \\  
\begin{figure}[bt!]
    \centering
    \includegraphics[width=9.5cm,height = 3.5cm]{gnn.png}
    \caption{Illustration of GNN training process based on node features}
    \label{learning}
\end{figure}
First, the GNN uses the training set to identify patterns between the features and labels of the nodes within that set. Subsequently, it predicts the labels of the nodes of the validation set based on their features. During this phase, the GNN adjusts its parameters depending on the accuracy of the predictions made. This includes updating the features of each node based on their adjacent nodes, illustrated in Figure \ref{learning}. Finally, the GNN predicts the label of each node of the test set, once more using the feature of the nodes. This process is then repeated for a certain amount of epochs. As the number of epochs increases, the GNN's accuracy improves, resulting in more accurate predictions overall. \\
Link prediction is the process of predicting whether there is an edge between two nodes in a graph or not. This task does not require additional sample data, such as the labels and features of the node classification task. In contrast, the input graph itself provides the sample data. A part of the existing edges in the graph are taken as positive examples. Similar, a portion of the non-edges are taken as negative examples. This data is used to minimize the prediction error of the GNN over a certain amount of epochs. Ultimately, the GNN is capable of predicting the existence of an edge or a non-edge between two nodes for all the potential edges in the graph that were not part of the sample data.
\section{Implementation}

This section illustrates how KaHIP was integrated into the DGL framework and what changes needed to be made to DGL to achieve that.
\subsection{Integration of KaHIP to DGL}
First of all, KaHIP was added to the third-party frameworks of DGL. After that, it was included in the building process, ensuring it gets compiled correctly. In DGL, the majority of complex functionalities are realized as follows: A file is created for each functionality, wherein a function is defined that can be called at a later stage outside of that individual file. Therefore, a new function was created that calls the main partitioner of KaHIP, named kaffpa. The kaffpa function requires all the connectivity information of the graph in order to work properly. In particular, the kaffpa function requires the amount of nodes in the graph as well as the source and destination node for each edge in the graph. This information was extracted by the respective DGL graph class utilizing the predefined methods of that class. Additionally, kaffpa must be configured. In essence, one of the six available modes of the function must be selected. The FAST mode was selected by default as a preliminary step. At that point in time, this was sufficient, as the objective was to initiate the operation of KaHIP. Subsequently, the detailed configurations were implemented at a later stage.\\
After this first crucial step was done, a crucial problem occurred. All the essential information of the graph is given to kaffpa in four arrays of integers. The conflict here was that DGL saves this information in 64-bit arrays, whereas the kaffpa function expects 32-bit arrays. To resolve this conflict, it was necessary to modify either DGL or KaHIP to accept the integer size of the other component, respectively. As modifying DGL would result in changing significantly more code, KaHIP was adjusted instead. This modification involved changing the function signatures of 21 functions inside of KaHIP, distributed across 10 files, as well as modifying the type definitions in its definition header to accept 64-bit integers for the ids of nodes and edges as well as their weights. These changes were successful in achieving their objective, whereby KaHIP could now be called successfully within DGL using 64-bit edges and nodes. \\
The next step was to include this new functionality in the corresponding partition utilities file. In this instance, an additional setup and a post-processing step are necessary before and after the actual partitioning function is called. This partitioning function was implemented as described above. The initial step is to transform the graph into a bidirectional graph. This means that for each $(u,v) \in E$ the corresponding backward edge $(v,u)$ is added to $E$. This is a standard intial step that is done before partitioning in DGL and is also done before every partition call of Metis. This process is illustrated in Figure \ref{bidirecting}. Then, after the partitioning process concludes, the halo nodes of each partition are identified. Subsequently, these nodes are added as to the partitions' halo nodes, respectively. This step in the implementation opened the possibility to call a KaHIP partition in DGL and then store the result and continue working on the different partitions at a later stage. \\
Even though a KaHIP partition could be conducted at this stage of development, the feature to actually utilize the partition in a distributed parallel context remained to be implemented. A function for this purpose had already been provided by DGL, but thus far only for a Metis partition call. The default behaviour of the function was to call a Metis partition. Each partition of the result was then stored in a destination directory in its own file. These individual partitions can then be loaded independently and then used by different computation units in parallel. The option of selecting either Metis or KaHIP when calling this function was enabled by adding an additional parameter that determines which partitioner to call. This concluded the integration of KaHIP into the DGL framework and each of the two partitioners could be called by the user.
\subsection{Addtitional Changes for Experiment}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=3.45cm,height = 2 cm]{directed_graph.png}
         \caption{Original graph}
         \label{uni}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=3.45cm,height = 2cm]{bidirectional_graph.png}
         \caption{Graph after adding backward edges}
         \label{bi}
     \end{subfigure}
     \caption{Process of bidirecting a graph}
     \label{bidirecting}
\end{figure}
Further features were added to the partitioning process of DGL to provide a more thorough analysis of the conducted experiments. Edge balancing is a crucial quality metric for the graph partitioning process. Accordingly, the subsequent objective was to add $deg(v) + 1$ as a weight to each node $n \in V$. This was accomplished by including an additional setup step before calling a partition, where the information of the outdegree was extracted from the graph. This was achieved through a function, provided by DGL, that returns a tensor. Each entry of that tensor represents the outdegree of a node in the graph. Then the value 1 was required to be added to each of those entries. This tensor could then be utilized by handing it to the partitioners by an additional parameter. As this method of edge balancing is a common one, the partitioners are able to accept these arrays and are able to return a partition that is balanced based on that array. This feature was introduced to investigate even further examine the influence of conventional partition quality metrics on the performance of GNNs. Moreover, a parameter was added to activate and deactivate this feature for different partitioning executions.  \\
KaHIP provides six distinct execution modes in total. The available modes are FAST, ECO, STRONG, FASTSOCIAL, ECOSOCIAL and STRONGSOCIAL.  At the current implementation stage, only the FAST mode was available, as previously stated in Section 4.1. To be able to choose from these modes, an additional parameter was introduced to all of the functions that include the KaHIP partitioning call. This allows for an insight into the impact of the different modes on different types of graphs in the experiment.

\section{Experiment}
\begin{table}[bt!]
\centering
\begin{tabular}{ ccc }
 \centering
  & Node classification & Link prediction \\ 
 \hline
 Distribution & single & fat \\
 CPUs & \multicolumn{2}{c}{Intel(R) Xeon(R) Gold 6230 CPU @ 2.10GHz} \\ 
 \# of CPUs & 32 & 32 \\ 
 Memory & 16GB & 356GB \\
\end{tabular}
\caption{Specifications of the used cluster}
\label{hw}
\end{table}
\begin{table}[bt!]
\centering
\begin{tabular}{ cccccc }
 \centering
 Graph & $|V|$ & $|E|$ & node feature size & \# of node categories \\ 
 \hline
 Questions \cite{TQ} & 49k & 307k & 301 & 2 \\ 
 Flickr \cite{Flickr} & 89k & 900k & 500 & 7\\ 
 Toloker \cite{TQ,Tolokers} & 12k & 1M & 10 & 2 \\

\end{tabular}
\caption{Graphs used for the experiment}
\label{graphs}
\end{table}
In this chapter, the experimental environment is described in detail. Afterwards, the results of the experiments are shown and evaluated. This includes a comparison of Metis and KaHIP performing a graph partition followed by the same subsequent training. The execution times of the training are compared for different partitioning sizes and vertex weights between Metis and all the available KaHIP execution modes. 
\subsection{Experimental Environment}
The experiment was run on the BwUniCluster2.0. For the node classification the single distribution and for the link prediction the fat distribution of the cluster was used respectively. Details of configurations of the cluster for the experiment is displayed in Table \ref{hw}.\\
In total, three graphs were used for the experiment. Their sizes, feature specifications and amount of different node categories are listed in Table \ref{graphs}. The original Toloker and Questions graph only have half of the edges shown in Table \ref{graphs}. DGL already included these two graphs into their example graph data sets and added all the backward edge for each edge on the original graph.\\
The two training types were executed as follows: First the graph is partitioned. Subsequently, the actual training for each partition is run in parallel using PyTorch \cite{PyTorch} and 100 epochs. PyTorch provides functions to run GNN training in parallel by creating a new process for each partition, which were utilized for the experiment. The training was repeated three times for each instance. Afterwards, the average of the three runs was determined and used for the diagrams of the following subchapters. Each graph was partitioned twice using the same configurations (for node classification and for link prediction). The average edge cut produced out of these two partition processes was taken for the edge cut results, shown in Figure \ref{cut}.
\subsection{Evaluation of Node Classification Results}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{nc_in_4.png}
         \caption{Partitioning time included}
         \label{nc_in_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{nc_ex_4.png}
         \caption{Partitioning time excluded}
         \label{nc_ex_4}
     \end{subfigure}
     \caption{Execution time of node classification using 4 partitions}
     \label{nc_results}
\end{figure}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{edge-cut_4.png}
         \caption{Edge cut for 4 partitions}
         \label{cut_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{edge-cut_8.png}
         \caption{Edge cut for 8 partitions}
         \label{cut_8}
     \end{subfigure}
     \caption{Produced edge cut of the algorithms}
     \label{cut}
\end{figure}
For this part of the experiment, the graphs were partitioned into four parts. The produced edge cut is listed in Figure \ref{cut_4}. In most cases, the KaHIP modes FAST and ECO resulted in the highest edge cut. Then, Metis and the modes FASTSOCIAL and ECOSOCIAL follow. The best edge cut was achieved by the STRONG and STRONGSOCIAL mode of KaHIP. However, these low edge cuts come with the cost of a very high partitioning time leading to poor execution times as illustrated in Figure \ref{nc_in_4}.\\
Here, the execution times are shown for every algorithm for each graph with and without vertex weights. Additionally, this illustration shows that overall the execution time of Metis and the FASTSOCIAL mode of KaHIP performed the best. \\
Figure \ref{nc_ex_4} shows the execution times as Figure \ref{nc_in_4}, but without including the time the partitioning algorithm run, respectively. This diagram illustrates the actual impact of the graph partitioning on the GNN training time. It shows that the training after a Metis partition is beaten by the STRONG, FASTSOCIAL, ECOSOCIAL and STRONGSOCIAL mode of KaHIP in most cases. These four modes achieved better edge cut quality overall than Metis. This shows, that the edge cut metric has a significant impact on GNN node classification training.\\
Another aspect this Figure \ref{lp_ex_4} illustrates is, that using vertex weights to balance the partitions has a significant positive impact on GNN training. 
\subsection{Evaluation of Link Prediction Results}
\begin{figure}[bt!]
     \centering
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3 cm]{lp_in_8.png}
         \caption{Partitioning time included}
         \label{lp_in_4}
     \end{subfigure}
     \hfill
     \begin{subfigure}[b]{0.45\textwidth}
         \centering
         \includegraphics[width=5cm,height = 3cm]{lp_ex_8.png}
         \caption{Partitioning time excluded}
         \label{lp_ex_4}
     \end{subfigure}
     \caption{Execution time of link prediction using 8 partitions}
     \label{lp_results}
\end{figure}
\section{Conclusion}
\section{Acknowledgments}
The author thanks Adil Chhabra, Ernestine Grossmann, Kenneth Langedal and Christian Schulz for supervising and supporting the project. Furthermore, the author acknowledges the support by the state of Baden-Württemberg through BwHPC.
\bibliographystyle{plainnat}
\bibliography{references.bib}

\end{document}
