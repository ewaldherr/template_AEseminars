\documentclass[acmsmall,nonacm,screen,review]{acmart}
\newif\ifEnableExtend
%\EnableExtendtrue
\EnableExtendfalse

\usepackage[utf8]{inputenc}
\usepackage{url}
\usepackage{color}
\newcommand{\csch}[1]{{\color{red} Christian says: #1}}
\newcommand{\Is}       {:=}
\newcommand{\set}[1]{\left\{ #1\right\}}
\newcommand{\sodass}{\,:\,}
\newcommand{\setGilt}[2]{\left\{ #1\sodass #2\right\}}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{xspace}
\usepackage{relsize}

\newtheorem{openproblem}{Open Problem}
\newcommand{\ie}{i.\,e.,\xspace}
\newcommand{\eg}{e.\,g.,\xspace}
\newcommand{\etal}{et~al.\xspace}
\newcommand{\cov}{\term{cov}\xspace}
\newcommand{\term}[1]{\textsl{#1}}
\newcommand{\Comment}[1]{\textsl{#1}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\setcopyright{none}
\copyrightyear{2024}
\acmYear{2024}
\acmDOI{}
\acmPrice{}
\acmISBN{}

\title{Algorithm Engineering Seminar: }
\author{Eric Waldherr}
\email{ft278@stud.uni-heidelberg.de, Computer Science,
4231309}
\affiliation{%
  \institution{Heidelberg University}
  \streetaddress{Im Neuenheimer Feld 205}
  \city{Heidelberg}
  \state{Baden-WÃ¼rttemberg}
  \country{Germany}
  \postcode{69120}
}


\date{10th of July 2024}

\begin{document}

\begin{abstract}
Insert abstract here.
\end{abstract}
\maketitle

\section{Introduction}
Over the last decades graphs have become a very important data structure that are used to save data
and run algorithms on in the modern era of computer science. Companies like Google, Facebook or Amazon use graphs with millions to billions of vertices and edges to save and compute data.\\ 
This trend of graphs getting larger and larger leads to the problem that one machine alone can not run computations on this huge data sets, because they just need to much memory. This is why the graph partitioning problem became more and more present over the time as these huge graphs need to be partitioned so that algorithms can run on multiple machines at a time. For that reason every algorithm that is computed on huge data sets and every parallel algorithm on graphs faces the graph partitioning problem at some point.\\ 
In the following chapters this problem is first explained in detail in Section 2, then there is an overview of different approaches on solving the problem is presented in Section 3. Finally there is a detailed explanation of the algorithm proposed in the paper which is also the main focus of this report in Section 4. At this point it should be noted that the most information used in this report is taken from that paper as well and this report is explaining and summarizing the paper. This report closes with a summary of the most important lessons that can be learned from this paper and this report.
\section{The Graph Partitioning Problem}
This chapter focuses on explaining the graph partitioning problem and giving the most important definitions needed to understand the following chapters.\\
Input of the problem is a Graph $G = (V,E)$. To solve the problem $G$ needs to be partitioned into $k\in \mathbb{N}$\ partitions $V_{1},...,V_{k}$ such that $V_{1}\cup...\cup V_{k} = V\ and\ V_{i}\cap V_{j} = \emptyset\ \forall i,j\in \{1,...,k\}$.\\
An important aspect is the edge cut quality, both with unweighted and weighted graphs. That means the partitioning algorithm needs to minimize 
$\sum c((u,v)) : u\in V_{i},\ v\in V_{j}\ and\ i\neq j$ \ or $\vert \{(u,v)\in E : u\in V_{i},\ v\in V_{j}\ and\ i\neq j \}\vert$ \ respectively.\\
As the partitioning is often done to prepare a graph for a parallel algorithm, these partitions need to be balanced. This means that $\forall i\in \{1,...,k\} : \vert V_{i}\vert \ < \lceil \frac{\vert V \vert }{k} \rceil $. This reduces the amount of communication needed between the different partitions, which is the amount of communication between different processing machines in the context of parallelization.
%TODO: Define edge/vertex imbalance
\section{Different Approaches to solve the graph partitioning problem}
The following subchapters give an overview of the most important approaches to solve the graph partitioning problem.
\subsection{Score based approach}
This approach divides the graph in k partitions at random initially.
Then, for every vertex the amount of edges inside the same partition and outside of its partition are computed. With that information it is clear whether a node should stay in its current partition or if it should be relocated into another partition. This process is repeated until no more relocations of vertices that would improve the quality of the partition can be found.\\
This approach has the problem that it gets stuck in a local optimum easily. In addition to that, there could be a beneficial swap of vertices for the edge cut quality even if the computation says otherwise.
\subsection{Multilevel approach}
This approach consists of three phases. \\
First there is the coarsening phase. This reduces the size of the graph by contracting connectivity information. All of this information needs to be saved in order to get the whole graph together again in the later stages of the algorithm. For that reason, one disadvantage of the multilevel approach is the high memory usage, which is important in the later chapters of this report.\\
Then the next phase begins, which is the initial partition step. Now the reduced graph is partitioned into k parts. After this step is done, what is left to do is to take the other vertices, that have been ignored so far, into consideration.\\
This is done in the final step, the uncoarsening phase. This step often includes heuristics and includes can include a swapping process that is similar to the swapping process of the score based approach. Now the original graph is reconstructed with the saved connectivity information. This phase ends with the whole graph partitioned into k partitions.
\subsection{Label Propagation approach}
A very detailed view of a LP approach will be given with the algorithm proposed in the paper. For that reason this chapter only describes the general LP approach in a high-level view.\\
Initially the graph is partitioned over the given machines. The LP approach consists of many iterations. In each iterations all vertices are assigned a score that describes, whether the node should be swapped into another partition or stay in the current partition. Then the vertices with an according score are relocated to another partition. This is often done until there is no more improvement to be found by using the scores.\\
The original LP approach does not guarantee a balanced partition. A balancing is often needed in modern applications making this fact a disadvantage of the original LP approach.
\section{Detailed description of a LP based algorithm}
This section focuses on the proposed algorithm of the paper. The algorithm is a LP based approach of solving the graph partitioning problem. The algorithm consists of three phases.\\
First the LP phase, which is called Quick converging LP (QCLP) in the paper. Its goal is conducting a LP phase, but only on the most important vertices to reduce the run time of the algorithm.\\
To avoid that the first phase got stuck in a local optima, the next phase, the stabilization phase, is run.\\
Finally to keep the final partition in the given vertex and edge balance constraints the last phase ensures that the constraints are fulfilled at the cost of edge cut quality.\\
In the following subchapters the initial computations of the algorithm as well as the three phases are explained in detail.
\subsection{Definitions} % maxbe number partitions?
First of all, there are a few definitions that are essential to understand the algorithm. \\
Every vertex has a vertex partition score (VP Score). The VP Score of vertex $v_{i}$ \ is defined as \[VP\ Score(v_{i})\ =\ v_{il}\ -\ v_{ir}\] where $v_{il}$ is the number of edges of $v_{i}$ to vertices inside of the same partition (local edges) and $v{ir}$ is the number of remote edges, which are edges to vertices of other partitions. Generally, the higher the VP Score of a vertex, the more appropriate it is in the current partition it is in. That is fact, because a high VP Score means a high amount of local vertices per definition. Relocating a such a vertex would result in a high increase of edge cut as all the edges to local vertices would be included in the cut.\\
The graph as a whole gets a score, the Total Score (T Score). This represents the quality of the current partition and is defined as follows. \[T\ Score\ =\ \sum_{v\in V} VP\ Score(v)  \]
It is notable that when the edge cut of the partition decreases, the T score increases, meaning that a higher T Score indicates a higher edge cut quality.\\
The algorithm only focuses on the most important vertices per step, called candidate vertices. In the first phase, the QCLP process, candidate vertices are the vertices that are inside of the set of local lower score vertices (LLSV). The LLSV of a partition $p_{i}$ is defined as \[LLSV(p_{i})\ =\ \{ m\in \mathbb{N}\ \text{vertices with the }m\text{-lowest scores in}\ p_{i}\}\] where $m$ is decreasing during the course of the algorithm. \\
During the second phase of the algorithm candidate vertices are vertices that are inside of the set of high connectivity to remote vertices (HCRV). HRCV of a partition $p_{i}$ is defined as 
\[HRCV(p_{i})\ =\ \{m\in \mathbb{N}\ \text{vertices with the }m\text{-highest amount of edges to remote vertices in}\ p_{i}\} \]
\subsection{Data structure}
It is really important to have the structure used in mind to understand the algorithm in detail. In this section the used data structure will be explained.\\
In general the algorithm uses two types of data, shared and local data. Initially the graph is randomly distributed among the machines used for the partition. The resulting information of the vertices and edges of each machine makes up the local data of the individual machine. It is important to state that this initial partition is immutable, meaning that once the vertices are distributed among the machines, they are not relocated to the local data of another machine, only the information that they swapped partitions is passed. The shared data, meaning information that is shared among all machines through communication, consists of the LLSV, the HCRV of each partition and a table that contains the information for each vertex, which partition it is currently in and which VP Score it is currently assigned.
\subsection{QCLP}
This process is the main stage of the algorithm. It uses the fact that relocating a vertex with a low VP score likely increases the edge cut quality. Essentially, this phase calculated the VP Scores and relocates nodes with low scores through many iterations and on each machine individually, until the edge cut is not improved noticeably.\\
Now, this phase will be looked at in detail. The first detail is in the calculation of the VP Score. Initially the VP Scores of all vertices is calculated. But in each following iteration only the VP Score of the vertices in LLSV and their neighbors is recalculated. This differs from the original LP approach, where the score of all vertices is recalculated each iteration. Here, this is only done on the stated vertices to safe computation time which is the bottleneck in this step of the algorithm. Also the calculation of the VP Score ignores vertices that are currently part of the LLSV. This is done as the vertices in the LLSV are going to be relocated soon. Thus including them into the calculation would lead to decisions made on data that will be outdated the next iteration.\\
Another important thing to take a look at is the communication structure used for this algorithm. It is called lazy-update BSP communication paradigm. This is supposed to solve the problem of the trade-off that is happening here. More communication would increase the communication overhead but at the same time improve the calculation of the VP Score, thus leading to a higher partition quality. This communication paradigm  results in the fact that the VP Scores are recalculated and shared if a vertex is in LLSV or in its neighborhood.\\
Also this phase is called Quick Converging Label Propagation because the size of LLSV is reduced every iteration, ensuring that this phase does not take too long overall. Each iteration of this phase works as follows. First the size of LLSV is decreased. Then the vertices for the next LLSV are found. Next, the LLSV vertices are relocated their new VP Score is calculated and this information is shared to the other workers. Then changes made of other machines are received and the VP Scores are reupdated based on that information. Finally, the local T Score is calculated and shared making up the overall T Score with the results of the other workers. This ends the current iteration and the next begins.
In general, this phase is done for at least $\alpha$ iterations. After that limit is reached, this phase continues until the T Score, calculated in the last step of each iteration, has not improved for $\beta$ iterations for $c$ percent. The parameters $\alpha,\beta,c$ are user specified.\\
In conclusion this phase does the most work for the partition and also keeps near perfect vertex balance in the process. However, it is important to note, that this phase does not lead to an optimal solution and in can end up getting trapped in a local optima. Also this phase does not ensure edge balancing at all. Both of these issues are addressed by the following phases of the algorithm.
\subsection{Stabilization phase}
This second phase of the algorithm is designed to resolve the potential problem, that the QCLP phase got trapped in a local optimum. This phase works similar to the QCLP. The main difference is that now nodes of the HRCV are relocated instead of nodes of the LLSV. The stabilization process also follows the same idea of the VP Score which is calculated as known of the QCLP. The difference is that now only vertices of VP Score less than zero are considered at all. And as stated in Section 4.1 the HRCV is made up of the vertices with the highest amount of remote vertices. But only vertices, where the partition they would be relocated to does not already exceed the edge/vertex imbalance ratio end up in the final HRCV. This imbalance ratio is now taken into consideration as this phase prevents this imbalance which was ignored by the QCLP gets even higher as in the final phase of the algorithm this imbalance will be fixed, which would likely lead to the undoing of a big amount of decisions made in this phase as the HRCV includes vertices with rather high amount of edges. This means that this phase has a huge impact on the edge imbalance.\\
Each iteration of this phase runs as follows. First the HRCV are made up as specified. Then they are relocated with their resulting VP Score, which is communicated to all machines. Then the information of the other machines is received including their respective relocations. Then the VP Scores are reupdated according to this new information. Finally, the T Score is calculated locally and then communicated to the other machines. This way the overall T Score can be updated. This phase runs through $\gamma$ iterations, which is user specified.
Overall, this phase keeps the near perfect vertex balance achieved in the QCLP while also preventing a further increasing edge imbalance. Also the local optima problem is resolved and the edge cut is improved even further.
\subsection{Edge balancing phase}
This final phase of the algorithm serves the purpose of keeping the edge/vertex imbalance restriction while also keeping the near perfect vertex balance achieved in the other phases. To achieve the needed edge balance this phase ignores the edge cut quality complete and focuses on the balance only. This phase works as follows. Partitions with too many edges relocate vertices randomly to partitions that are below the edge limit in descending order of outdegree. This process is repeated until the edge balance is fulfilled. Therefore, this phase has a negative impact on the edge cut as this process completely ignores the edge cut. However, this ensures that all balancing restrictions are fulfilled. This marks the end of the algorithm.
\section{Evaluation of the algorithm}
The paper the described algorithm is proposed in also includes several experiments and comparisons to other partitioners. In this section, first the experiment structure will be explained. Then the results of that experiments will be presented and finally ideas that could improve the algorithm will be presented.
\subsection{Experiment}
First of all the other partition frameworks used for the experiment will be presented. Metis and ParMetis are known high quality partitioners. These partitioners are used in a variety of experiments as they are known for they speed and high quality. While Metis only works on single machines, ParMetis is the parallel version of Metis. They are used here as they are popular partitioners that use the multilevel approach, described in Section 3.2. This provides an excellent comparison to the very wildly spread multilevel approach. In addition to that, XtraPuLP, which is a parallel version of PuLP, is used to provide a comparison to another LP based partition approach. All of the partitioners were run with a vertex imbalance of 1.03 and a edge imbalance of 1.30.\\
Furthermore, two different clusters were used for the experiment, a smaller one and a large one. Also, eight different graphs of different sizes were used in total. The seven smaller graphs were partitioned on the small cluster while the three largest graphs were run on the large cluster.\\
Looking at the execution time of the algorithms first, it is clear, that the proposed algorithm runs fasted, followed by XtraPuLP and then with a significant distance Metis and ParMetis. Another thing that can be noticed immediately is that Metis and ParMetis did not compute for the two largest graphs run on the small cluster. This and the high difference in runtime can be explained by the fact that the multilevel approach is very memory consuming, as described in Section 3.2. The clusters provide only very limited amount of memory per vertex leading to the measured struggle of the two multilevel partitioners. Additionally, the proposed algorithm is the only framework able to complete the partition of the largest graph on the small cluster, which is not shown in the figures. \\
Taking a look at the edge cute quality it becomes clear, that the proposed algorithm is close to the quality of Metis and is outperforming XtraPuLP for the bigger graphs. This shows that the algorithm is capable of producing high quality partitions.\\
Looking at the balancing restrictions it is clear that ParMetis is the only partitioner that exceeds the limits significantly while XtraPuLP shows the same behavior at the edge balancing. Apart from that all partitioners more or less were able to fulfill the restrictions.\\
Overall the experiments on the small cluster show the excellent scalability of the proposed algorithm as it is outperforming the other partitioners on the limited resources of the small cluster. To achieve a high scalability is also the main focus of the algorithm marking this as a great success of development.\\
When looking at the experiments on the larger cluster you notice, that the results are very similar in the field of execution time and balancing. Taking the fact into consideration that the memory per vertex on the large cluster is still very small, it is clear why Metis and ParMetis perform very poorly in terms of runtime. While having similar edge cut as Metis on the large cluster as well, the proposed algorithm shows lower edge quality than XtraPuLP on the largest graph. 
\subsection{Evaluation and possible improvements}
The results of the experiment make clear that the main goal of the proposed algorithm, which was producing a partitioner for large-scale graph data, was successful. Especially on clusters with limited memory resources the proposed algorithm provides excellent execution time and balancing and outperforms other partitioners while keeping a high edge cut quality. A comparison on clusters that are more sufficient for the multilevel approach would be very interesting and could show whether the proposed algorithm is outperforming other partitioners in that regard as well. However, there could be issues for very large data for the edge cut quality as it can be seen in the results for the largest graph where XtraPuLP showed a higher quality.\\
This leads to possible improvements that could be made on the algorithm. The edge balancing phase of the algorithm seems to be the best point for improvement as during this phase the edge cut property is completely ignored. While this makes that phase really fast, the edge cut can suffer severely during this phase. Improving that phase would mean slowing it down, but looking at the excellent execution time of the algorithm, a trade-off increasing the runtime but also increasing the edge cut quality should be taken into consideration.\\
Concretely an additional execution mode should be introduced for the algorithm. That way the user could decide whether the very fast normal mode or a mode with increased edge cut should be run. As described in Section 4.3 this phase only sorts vertices in partitions with too many edges by outdegree and relocates them randomly in descending order of outdegree. This additional mode could take the latest VP Score into consideration, that was communicated during the stabilization phase. That way there should be a calculation introduced that looks at the vertices with the highest outdegree and checks, if that vertex has a high VP Score, if it is beneficial to relocate other vertices with high outdegrees instead. Overall this could fix the issue of low edge cut quality seen in the experiment on the largest graph, while increasing the runtime.  
\section{Conclusion}
All in all the paper introduces a partitioner that performs really well on clusters with low memory resources. Also, the quick runtime surely is an argument for using this algorithm in all kinds of applications where a fast partitioner is needed. In addition to that, the algorithm is able to fulfill given balancing restraints. In the future, improvements to the produces egde cut of the algorithm could produce even more use cases for this framework. 
\bibliographystyle{plainnat}
\bibliography{references.bib}

\end{document}
